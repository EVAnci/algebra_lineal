\section{Funciones Lineales}

Una función lineal (también llamada transformación lineal o aplicación lineal) es una función entre dos espacios vectoriales que preserva la estructura algebraica de los vectores, es decir, respeta la suma de vectores y la multiplicación por escalares.

\textbf{Definición}: Sea \(f: V \to U\) una función entre espacios vectoriales sobre el mismo cuerpo \(K\). 
\begin{align*}
  f: V &\rightarrow U\\
  v &\rightarrow u
\end{align*}
que a cada vector \(v\) de \(V\) le hace corresponder un único vector \(f(v)\) de \(U\).

Diremos que \(f\) es una aplicación lineal si para todo \(v, w \in V\) y todo escalar \(t \in K\), se cumple:
\begin{itemize}
  \item \(f(v + w) = f(v) + f(w)\)
  \item \(f(t \cdot v) = t \cdot f(v)\)
\end{itemize}

La primera condición se la conoce como \textit{aditividad}, indica que la función respeta las operaciones internas definidas en los espacios vectoriales involucrados, la segunda se denomina \textit{homogeneidad} y está indicando que la función también respeta las operaciones externas. 

\begin{tcolorbox}[remember, title=Aclaración]
  \(V(K)\) y \(U(K)\) son dos espacios vectoriales definidos sobre un mismo cuerpo \(K\) (por ejemplo, \(\mathbb{R}\) o \(\mathbb{C}\)).
  
  La función \(f: V \rightarrow U\) toma un vector \(v \in V\) y lo lleva a un vector \(f(v) \in U\).
  
  Es importante entender que tanto el dominio como el codominio son espacios vectoriales, por lo tanto, en ambos hay operaciones de suma y producto por escalar.
\end{tcolorbox}

En otras palabras, una función \(f: V \rightarrow U\) es lineal si cumple dos condiciones fundamentales para todo \(v, w \in V\) y \(t \in K\):
\begin{enumerate}
  \item Aditividad (respeta la suma):
    \[
     f(v + w) = f(v) + f(w)
    \]
  \item Homogeneidad (respeta el producto por escalares):
    \[
      f(t \cdot v) = t \cdot f(v)
    \]
\end{enumerate}

Estas dos propiedades garantizan que la función ``conserva'' la estructura lineal. En otras palabras, las combinaciones lineales en el espacio de partida se transforman en combinaciones lineales en el espacio de llegada, de la misma forma.

\paragraph{Propiedades de una función lineal}

Si \(f\) es una función lineal de \(V(K)\) en \(U(K)\) entonces:
\begin{enumerate}
  \item \(f\left(\vec{0}\right) = \vec{0}, \qquad \left(\vec{0} \in V,\vec{0} \in U\right)\)

  Demostración:\[
    f\left(\vec{0}\right) = f\left(0 \cdot v\right) = 0 \cdot f(v) = \vec{0}
  \]
  \item \(f(-v)=-f(v), \quad \forall v \in V\)
  
  Demostración: \[
    f(-v) = f(-1 \cdot v) = -1 \cdot f(v) = -f(v)
  \]
  \item \(f(v-w)=f(v) - f(w) \quad \forall v,w \in V\)
  
  Demostración: \[
    f(v-w) = f(v) + f(-w) = f(v) + (-f(w)) = f(v) - f(w)
  \]
\end{enumerate}

\begin{tcolorbox}[interesting_data, title=Nota conceptual]
  Las propiedades desarrolladas son sumamente importantes. Por ejemplo la primer propiedad distingue claramente las funciones lineales de otras funciones. Por ejemplo, si una función \(f\) no cumple que \(f\left(\vec{0}\right) = \vec{0}\), entonces automáticamente no es lineal.
\end{tcolorbox}

Ahora para todo par de escalares \(a,b \in K\) y para todo par de vectores \(v,w \in V\), obtenemos imponiendo ambas condiciones de linealidad:
\[
  F(av + bw) = a\cdot f(v) + b\cdot f(w) \qquad \text{se utiliza para definirlas}
\]
Con mayor generalidad, para escalares \(a_i \in K\) y vectores \(v_i \in V\), llegamos:
\[
  F(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n) = a_1 f(v_1) + a_2 f(v_2) + \cdots + a_n f(v_n)
\]
\begin{tcolorbox}[myconclusion]
  Expresión utilizada para demostrar los teoremas.
\end{tcolorbox}

\begin{quote}
  \ejemplo{ Veamos algunos ejemplos}
  \begin{enumerate}[label=\alph*.]
    \item Sea \(A\) cualquier matriz \(m \times n\) sobre un cuerpo \(K\). Como se señaló previamente, \(A\) determina una aplicación \(F:K^n \rightarrow K^m\) mediante la asignación \(v \rightarrow Av\) (aquí los vectores de \(K^n\) y \(K^m\) se escriben como columnas). Afirmamos que \(F\) es lineal. Esto es debido a que, por propiedades de las matrices:
    \begin{align*}
      F(v + w) &= A(v+w) = Av + Aw = F(v) + F(w) \\
      F(kv) &= A(kv) = kAv = kF(v)
    \end{align*}
    donde \(v, w \in K^n\) y \(k \in K\).
    \item Sea \(F:\mathbb{R}^3 \rightarrow \mathbb{R}^3\) la aplicación <<proyección>> en el plano \(xy: F(x,y,z)= (x,y,0)\). Probemos que \(F\) es lineal. Sean \(v=(a,b,c)\) y \(w=(a',b',c')\). Entonces:
    \begin{align*}
      F(v+w) &= F(a+a', b+b', c+c') = (a+a', b+b',0) = \\
            &= (a,b,0) + (a',b',0) = F(v) + F(w)
    \end{align*}
    y para todo \(k \in \mathbb{R}\):
    \[
      F(kv) = F(ka,kb,kc) = (ka,kb,0) = k(a,b,0) = kF(v)
    \]
    O sea, \(F\) es lineal.
    \item Sea \(F: \mathbb{R}^2 \rightarrow \mathbb{R}^2\) la aplicación de <<traslación>> definida según \(F(x,y) = (x+1,y+2)\). Obsérvese que \(F(0)=(0,0)=(1,2)\neq 0\). Es decir, el vector cero no se aplica sobre el vector cero. Por consiguiente \(F\) no es lineal.
    \item Sea \(F:V\rightarrow U\) la aplicación que asigna \(0 \in U\) a todo \(v \in V\). Para todo par ded vectores \(v,w \in V\) y todo \(k \in K\) tenemos:
    \[
      F(v+w) = 0 = 0+0 = F(v) + F(w) \qquad \text{y} \qquad F(kv) = 0 = k0 = kF(v)
    \]
    Así \(F\) es lineal. Llamamos a \(F\) la \textit{aplicación cero} y la denotaremos normalmente por \(0\).
    \item Consideremos la aplicación identidad \(I:V\rightarrow V\) que aplica cada \(v\in V\) en si mismo. Para todo par de vectores \(v, w \in V\) y todo par de escalares \(a,b \in K\):
    \[
      I(av+bw)= av + bw = aI(v) + bI(w)
    \]
    Así \(I\) es lineal.
    \item Sea \(V\) el espacio vectorial de los polinomios en la variable \(t\) sobre el cuerpo real \(\mathbb{R}\). La aplicación derivada \(\mathbf{D}:V\rightarrow V\) y la aplicación integral \(\mathbf{J}:V\rightarrow \mathbb{R}\), son lineales. La razón es que, según se demuestra en el cálculo, para todo par de vectores \(u,v \in V\) y todo \(k\in \mathbb{R}\):
    \[
      \frac{d(u+v)}{dt} = \frac{du}{dt} + \frac{dv}{dt} \qquad \text{y} \qquad \frac{ku}{dt} = k\frac{du}{dt}
    \]
  \end{enumerate}
 \end{quote}

\subsubsection{Clasificación de las funciones lineales}

Se tiene una función lineal \(f\) de \(V(K)\) en \(W(K)\):
\begin{itemize}
  \item Si \(f\) es inyectiva recibe el nombre de \textit{monomorfismo}
  \item Si \(f\) es suryectiva recibe el nombre de \textit{epimorfismo}
  \item Si \(f\) es biyectiva recibe el nombre de \textit{isomorfismo}
  \item Si en \(f \quad V=W\) recibe el nombre de \textit{endomorfismo}
  \item Si \(f\) es un endomorfismo y es biyectiva recibe el nombre de \textit{automorfismo} 
\end{itemize}

\subsubsection{Núcleo e imagen de una función lineal}

Sea \(f\) una función lineal de \(V(K)\) en \(U(K)\) \(\left(f:V\rightarrow U\right)\), asociados a esta función existen dos subconjuntos, uno del conjunto de partida y otro del conjunto de llegada, que reciben el nombre de \textit{núcleo} (o \textit{Ker}) de la función e imagen de la función, los definimos:
\[
\text{Ker } f = N(f) = \left\{ v \in V \mid f(v) = \vec{0}_U \right\}
\]
\begin{center}
  son todos los vectores de \(V\) que tienen como imagen al vector nulo de \(U\)
\end{center}
\[
\text{Im } f = \left\{u\in U \mid \exists v \in V ~\text{ tal que }~ f(v) = u \right\}
\]
o también en una forma más abreviada:
\[
\text{Im } f = \left\{f(v) \mid v \in V\right\}
\]
\begin{center}
  son todos los vectores de \(U\) que son imagen de algún vector \(v \in V\)
\end{center}

Los subconjuntos \(N(f)\) e \(\text{Im } f\) son subespacios de \(V\) y de \(U\) respectivamente.

\begin{quote}
  \ejemplo{ Consideremos la siguiente función lineal:}
  \[
    f(x) = 3x
  \]
  Esta es una función lineal \(f:\mathbb{R} \rightarrow \mathbb{R}\).
  \begin{itemize}
    \item \textbf{Núcleo:}
    \[
      \text{Ker } f = \left\{x \in \mathbb{R} \mid f(x) = 0\right\} = \{0\}
    \]
    \item \textbf{Imagen:}
    \[
      \text{Im } f = \left\{f(x) \mid x \in \mathbb{R}\right\} = \left\{3x \mid x \in \mathbb{R}\right\} = \mathbb{R}
    \]
  \end{itemize}
  Aquí el \textit{único} elemento que se anula es el \(0\), pero la imagen es \textit{todo} \(\mathbb{R}\).

  \vspace{5mm}

  \ejemplo{ Ahora consideremos la función trivial de \(\mathbb{R}\) a \(\mathbb{R}\):}
  \[
    f(x) = 0
  \]
  Esta función también es lineal, y \(f:\mathbb{R} \rightarrow \mathbb{R}\)
  \begin{itemize}
    \item \textbf{Núcleo:}
    \[
      \text{Ker } f = \left\{x \in \mathbb{R} \mid f(x) = 0\right\} = \mathbb{R}
    \]
    \item \textbf{Imagen:}
    \[
      \text{Im } f = \{0\}
    \]
  \end{itemize}
  En este caso, \textit{todo} el dominio se anula y la imagen es el conjunto reducido que contiene solo al cero.
\end{quote}

\begin{tcolorbox}[title=Resumen para fijar la idea]
  \begin{itemize}
    \item El \textbf{núcleo} es un subconjunto del \textbf{dominio} \(V\): son los vectores que van a parar al cero del codominio.
    \item La \textbf{imagen} es un subconjunto del codominio \(U\): son todos los valores posibles que puede tomar \(f(v)\)
  \end{itemize}

  \vspace{5mm}

  El núcleo responde a la pregunta: ``¿Qué vectores se anulan bajo \(f\)?''

  La imagen responde a: ``¿Qué vectores pueden obtenerse como salida de \(f\)''
\end{tcolorbox}

\subsubsection{Los subespacios Núcleo y Imagen de \(f\)}

\paragraph{El conjunto núcleo de \(f\) es un subespacio del dominio}

\textbf{Proposición}: Sea \(f: V \to U\) una aplicación lineal entre espacios vectoriales. Entonces, el conjunto núcleo de \(f\),
\[
\text{Ker } f = \{ v \in V \mid f(v) = 0 \}
\]
es un subespacio vectorial de \(V\).

\textit{Demostración}
\begin{quote}
  \begin{enumerate}
    \item \textbf{No vacío}:
    
    El vector nulo del dominio \(\vec{0}_V \in V\) cumple que:
      \[
      f(\vec{0}_V) = \vec{0}_U
      \]
    por lo tanto, \(\vec{0}_V \in \text{Ker } f\), y así \(\text{Ker } f \ne \emptyset\).
  
    \item \textbf{Cerrado bajo la suma}:
    
      Sean \(v_1, v_2 \in \text{Ker } f\). Entonces:
     \[
     f(v_1) = 0 \quad \text{y} \quad f(v_2) = 0
     \]
     Como \(f\) es lineal:
     \[
     f(v_1 + v_2) = f(v_1) + f(v_2) = 0 + 0 = 0
     \]
     Entonces \(v_1 + v_2 \in \text{Ker } f\).
  
    \item \textbf{Cerrado bajo el producto por escalares}:
     
      Sea \(t \in K\) y \(v \in \text{Ker } f\), es decir, \(f(v) = 0\). Entonces:
     \[
     f(t \cdot v) = t \cdot f(v) = t \cdot 0 = 0
     \]
     Por lo tanto, \(t \cdot v \in \text{Ker } f\).
  \end{enumerate}
  
  Conclusión: Se cumplen las tres condiciones necesarias para que \(\text{Ker } f\) sea un subespacio de \(V\).
  \(\blacksquare\)
\end{quote}

\paragraph{El conjunto imagen de \(f\) es un subespacio del codominio}


\textbf{Proposición}: Sea \(f: V \to U\) una aplicación lineal entre espacios vectoriales sobre un mismo cuerpo \(K\). Entonces el conjunto
\[
\text{Im } f = \{ u \in U \mid \exists v \in V \text{ tal que } f(v) = u \}
\]
es un subespacio vectorial de \(U\).

\textit{Demostración:}
\begin{quote}
  \begin{enumerate}
    \item \textbf{No vacío}:
      Como \(f\) es lineal, se cumple que:
      \[
      f(\vec{0}_V) = \vec{0}_U
      \]
      Entonces, \(\vec{0}_U \in \text{Im } f\), por lo tanto \(\text{Im } f \ne \emptyset\).

    \item \textbf{Cerrado bajo suma y producto por escalares} (en una sola propiedad):

      Sean \(u_1, u_2 \in \text{Im } f\).
      Por definición, existen \(v_1, v_2 \in V\) tales que:
      \[
      f(v_1) = u_1 \quad \text{y} \quad f(v_2) = u_2
      \]
      Sean \(a, b \in K\) escalares arbitrarios. Como \(f\) es lineal:
      \[
      f(a v_1 + b v_2) = a f(v_1) + b f(v_2) = a u_1 + b u_2
      \]
      Esto significa que \(a u_1 + b u_2 \in \text{Im } f\).
  \end{enumerate}

  Conclusión: La imagen de \(f\) cumple las condiciones necesarias para ser subespacio de \(U\): no es vacía, y es cerrada bajo combinaciones lineales.
  \(\blacksquare\)
\end{quote}

\begin{tcolorbox}[title=Observaciones]
  Note que no necesitamos comprobar ``cerrado bajo suma'' y ``cerrado bajo producto escalar'' por separado, ya que probar cerrado bajo combinaciones lineales es más general y suficiente.
\end{tcolorbox}


%%%% A partir de aquí he copiado todo tal y como sale en el apunte de artal. Luego lo modificaré.

Ejemplos
\label{ej:ejemplos_a_revisar}
\begin{enumerate}[label=\alph*.]
  \item Sea \(F:\mathbb{R}^3 \rightarrow \mathbb{R}^3\) la aplicación proyección sobre el plano \(xy\). Esto es,
  \[
    F(x,y,z)=(x,y,0)
  \]
  (Véase la figura {insertar figura aquí}) Claramente la imagen de \(F\) es todo el plano \(xy\). Es decir, 
  \[
    \text{Im } F = \left\{(a,b,c):c\in \mathbb{R}\right\}
  \]
  Nótese que el núcleo de \(F\) es el eje \(z\). O sea,
  \[
    \text{Ker } F = \left\{(0,0,c):c\in \mathbb{R}\right\}
  \]
  ya que estos puntos y solamente éstos se aplican en el vector cero \(0=(0,0,0)\).
  \item Sean \(V\) el espacio vectorial de los polinomios sobre \(\mathbb{R}\) y \(\mathbf{T}:V\rightarrow V\) el operador tercera derivada, esto es,
  \[
    \mathbf{T}\left[f(t)\right] = \frac{d^3 f}{dt^3}
  \]
  A veces se emplea la notación \(\mathbf{T}=\mathbf{D}^3\), donde \(\mathbf{D}\) es la aplicación derivada. Tendremos:
  \[
    \text{Ker } T = \left\{\text{polinomios de grado} \leq 2\right\}
  \]
  porque \(\mathbf{T}(at^2+bt+c)=0\) pero \(\mathbf{T}(t^n)\neq 0\) para \(n > 3\). Por otra parte
  \[
    \text{Im } T = V
  \]
  puesto que todo polinomio en \(V\) es la tercera derivada de algún polinomio.
\end{enumerate}

\paragraph{Teorema de la imagen de un sistema de generadores}

Supongamos ahora que los vectores \(v_1, \cdots, v_n\) generan \(V\) y que \(F:V\rightarrow U\) es lineal. Probemos que los vectores \(F(v_1),\cdots, F(v_n)\in U\) generan \(\text{Im } F\); en ese caso, \(F(v)=u\) para algún vector \(v\in V\). Como los \(v_i\) generan \(V\) y \(v \in V\), existen escalares \(a_1, \cdots, a_n\) tales que:
\[
  v = a_1 v_1 + a_2 v_2 + \cdots + a_n v_n
\]  
En consecuencia:
\[
  u = F(v) = F(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n) = a_1 F(v_1) + a_2 F(v_2) + \cdots + a_n F(v_n)
\]
y por lo tanto los vectores \(F(v_1), \cdots , F(v_n)\) generan \(\text{Im } F\).

Establezcamos formalmente este útil resultado.

Supongamos que \(v_1, v_2, \cdots, v_n\) generan un espacio vectorial \(V\) y que \(F:V\rightarrow U\) es lineal.

Entonces \(F(v_1), F(v_2), \cdots, F(v_n)\) generan \(\text{Im } F\).

\subsection{Rango y nulidad de una aplicación lineal}

Hasta aquí no hemos relacionado la noción de dimensión con la de aplicación lineal \(F : V \rightarrow U\). En pos casos enq ue \(V\) es de dimensión finita, tenemos la relación fundamental que sigue:

\teorema
Sea \(V\) de dimensión finita y sea \(F:V\rightarrow U\) una aplicación lineal. Entonces:
\begin{equation}
  \dim V = \dim( \text{Ker } F) + \dim(\text{Im } F)
  \label{eq:teorema_dimensión_nucleo_imagen}  
\end{equation}

El teorema proporciona, la siguiente fórmula para \(F\) cuando \(V\) tiene dimensión finita:

Es decir, la suma de las dimensiones de la imagen y el núcleo de una aplicación lineal es igual a la dimensión de su dominio. 

Se ve fácilmente que la ecuación (?9.1) se rige para la aplicación proyección de \(F\) del ejemplo (?9.5a - creo que es el primer ejemplo de \ref{ej:ejemplos_a_revisar}). Allí la imagen (plano \(xy\)) y el núcleo (eje \(z\)) de \(F\) tienen dimensiones 2 y 1, respectivamente, mientras que el dominio \(\mathbb{R}^3\) de \(F\) tiene dimensión 3.

\textbf{Nota}: Sea \(F:V\rightarrow U\) una aplicación lineal. Se define el \textit{rango} de \(F\) como la dimensión de su imagen y la \textit{nulidad} de \(F\) como la dimensión de su núcleo; esto es:
\[
  \text{rango } F = \dim(\text{Im } F) \qquad \text{y} \qquad \text{nulidad } F = \dim(\text{Ker } F)
\]
\[
\text{rango } F + \text{nulidad } F = \dim V
\]
Recordemos que el rango de una matriz se definió en origen como la dimensión de su espacio columna y de su espacio fila. Obsérvese que si ahora \(A\) como una aplicación lineal, las dos definiciones se corresponden, porque la imagen de \(A\) es precisamente su espacio columna.

Ejemplo

Sea \(F:\mathbb{R}^4 \rightarrow \mathbb{R}^3\) la aplicación lineal definida por:
\[
  F(x,y,s,t) = (x-y+s+t,x+2s-t,x+y+3s-3t)
\]
\begin{enumerate}[label=\alph*.]
  \item Encontramos una base y la dimensión de la imagen de \(F\).
  
  Hallamos la imagen de los vectores de la base usual de \(\mathbb{R}^4\):
  \begin{align*}
    F(1,0,0,0) = (1,1,1)\phantom{-} &\qquad F(0,0,1,0) = (1,2,3)\\
    F(0,1,0,0) = (-1,0,1) &\qquad F(0,0,0,1) = (1,-1,-3)
  \end{align*}
  Según la proposición (??) (revisar), los vectores imagen generan \(\text{Im } F\); por eso construimos la matriz cuyas filas son estos vectores imagen y la reducimos por filas a forma escalonada:
  \[
  \begin{pmatrix}
    1 & 1 & 1 \\
    -1 & 0 & 1 \\
    1 & 2 & 3 \\
    1 & -1 & -3
  \end{pmatrix} \sim \begin{bmatrix}
    1 & 1 & 1 \\
    0 & 1 & 2 \\
    0 & 1 & 2 \\
    0 & -2 & -4
  \end{bmatrix} \sim \begin{bmatrix}
    1 & 1 & 1 \\
    0 & 1 & 2 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
  \]
  De este modo, \((1,1,1)\) y \((0,1,2)\) constituyen una base de \(\text{Im } F\), luego \(\dim(\text{Im } F) = 2\) o, en otras palabras, rango \(F=2\).
  \item Encontraremos una base y la dimensión del núcleo de la aplicación \(F\).

  Hacemos \(F(v)=0\), donde \(v=(x,y,z,t)\):
  \[
    F(x,y,s,t) = (x-y+s+t, x+2s-t, x+y+3s-3t) = (0,0,0)
  \]
  Igualamos entre si las componentes correspondientes para formar el siguiente sistema homogéneo cuyo espacio solución es \(\text{Ker } F\):
  \begin{align*}
    \begin{cases}
      x - y + \phantom{1}s + \phantom{1}t = 0 \\
      x \phantom{+ 1y } + 2s - \phantom{1}t = 0 \\
      x + y + 3s - 3t = 0
    \end{cases} \quad \text{o} \quad \begin{cases}
      x - y + \phantom{1}s + \phantom{1}t = 0 \\
      \phantom{x +} \phantom{1}y + \phantom{1}s - 2t = 0 \\
      \phantom{x +} 2y + 2s - 4t = 0
    \end{cases} \quad \text{o} \quad \begin{cases}
      x-y+s+\phantom{1}t=0\\
      \phantom{x+} y+s-2t = 0
    \end{cases}
  \end{align*}
  Las variables libres son \(s\) t \(t\), luego \(\dim(\text{Ker } F)=2\) o \(\text{nulidad } F =2\). Tomamos:
  \begin{itemize}
    \item \(s=-1,t=0\), para obtener la solución \((2,1,-1,0)\).
    \item \(s=0,t=1\), para obtener la solución \((1,2,0,1)\).
  \end{itemize}
  Así \((2,1,-1,0)\) y \(1,2,0,1\) constituyen una base de \(\text{Ker } F\). (Obsérvese que \(\text{rango } F + \text{nulidad } F = 2 + 2 = 4\)), que es la dimensión del dominio \(\mathbb{R}^4\) de \(F\).
\end{enumerate}

\subsection{Aplicación a los sistemas de ecuaciones lineales}

Consideremos un sistema de \(m\) ecuaciones lineales con \(n\) incógnitas sobre un cuerpo \(K\):
\begin{align*}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n &= b_1 \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n &= b_2\\
  \vdots  \qquad \qquad ~ & \\
  a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n &= b_m
\end{align*}
que es equivalente a la ecuación matricial
\[
  Ax=b
\]
donde \(A=(a_{ij})\) es la matriz de los coeficientes y \(x=(x_i)\) y \(b=(b_i)\) los vectores columna de las incógnitas y de las constantes, respectivamente. Ahora la matriz \(A\) puede verse también como la aplicación lineal 
\[
  A:K^n \rightarrow K^m
\]
Siendo así, la solución de la ecuación \(A:K^n \rightarrow K^m\). Más aún, la solución de la ecuación homogénea asociada \(Ax=0\) puede verse como el núcleo de la aplicación lineal \(A:K^n \rightarrow K^m\).

El teorema \ref{eq:teorema_dimensión_nucleo_imagen} relativo a aplicaciones lineales nos conduce a la relación:
\[
  \dim(\text{Ker } A = \dim(K^n) - \dim(\text{Im } A) = n - \text{rango } A)
\]
Pero \(n\) es exactamente el número de incógnitas en el sistema homogéneo \(Ax=0\).

\subsection{Aplicaciones lineales singulares y no singulares (isomorfismo)}

Se dice que una aplicación lineal \(F: V \rightarrow U\) es \textit{singular} si la imagen de algún vector no nulo bajo \(F\) es \(0\), es decir, si existe algún \(v \in V\) tal que \(v \neq 0\) pero \(F(v) = 0\). De esta manera, \(F:V \rightarrow U\) es \textit{no singular} si unicamente \(0 \in V\) se aplica en \(0 \in U\) o, equivalentemente, si su núcleo consiste solamente en el vector cero: \(\text{Ker } F = \left\{0\right\}\).

\teorema Supongamos que una aplicación lineal \(F: V \rightarrow U\) es no singular. En tal caso, la imagen de cualquier conjunto linealmente independiente es linealmente independiente.

\textbf{Demostración}: Supongamos que \(v_1, v_2, \cdots, v_n\) son vectores linealmente independientes en \(V\). Afirmamos que \(F(v_1),F(v_2),\cdots, F(v_n)\) son también linealmente independientes. Supongamos que:
\[
  a_1 F(v_1) + a_2 F(v_2) + \cdots + a_n F(v_n) = 0 \quad \text{donde } a_i ~ \text{pertenece a } K
\]
dado que \(F\) es lineal \(F(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n) = 0\) por lo tanto,
\[
  \left(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n\right) \in \text{Ker } F
\]
Pero \(F\) es no singular, esto es el \(\text{Ker }F = \{0\}\), luego \(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n = 0\). Siendo linealmente independientes los \(v_i\), todos los \(a_i\) son cero. De acuerdo con esto los \(F(v_i)\) son linealmente independientes.

\paragraph{Isomorfismos}

Supongamos que una aplicación lineal \(F:V \rightarrow U\) es inyectiva. Entonces solo \(0 \in V\) puede aplicarse en \(0 \in U\) y por tanto \(F\) es no singular. El recíproco es cierto también, porque si \(F\) es no singular y \(F(v)=F(w)\), necesariamente \(F(v-w) = F(v) - F(w) = 0\), luego \(v-w = 0\) o \(v=w\). Así \(F(v) = F(w)\) implica \(v=w\), o sea, \(F\) es inyectiva. Hemos demostrado pues,

\textbf{Proposición}: Una aplicación lineal \(F:V\rightarrow U\) es inyectiva si y solo si es no singular.

Recordemos que una aplicación \(F:V\rightarrow U\) recibe el nombre de isomorfismo si es lineal y biyectiva, esto es, si es lineal, inyectiva y suprayectiva. Recordemos, asimismo, que se dice que un espacio vectorial \(V\) es isomorfo a otro \(U\), escrito \(V \simeq U\), si existe algún isomorfismo \(F:V\rightarrow U\).

En este caso es aplicable el siguiente teorema:
\begin{quote}
  Supongamos que \(V\) tiene dimensión finita y que \(F:V \rightarrow U \) es lineal. Entonces \(F\) es un isomorfismo si y solo si no es singular.

  Demostración:
  Si \(F\) es un isomorfismo, solo \(0\) se aplica en \(0\), de modo que \(F\) es no singular. Supongamos que \(F\) es no singular. Entonces \(\dim(\text{Ker } F) = 0\). Según el teorema (libro:9.5??), \(\dim(V) = \dim(\text{Ker } F) + \dim(\text{Im } F)\).
  Así pues, \(\dim(U) = \dim(V) = \dim(\text{Im } F)\). Como \(U\) tiene dimensión finita, \(\text{Im } F = U\) y por ende \(F\) es suprayectiva. Por lo tanto, \(F\) es simultáneamente inyectiva y suprayectiva, es decir \(F\) es un isomorfismo.
\end{quote}

\subsection{Operaciones con aplicaciones lineales}

Podemos combinar aplicaciones lineales de varias maneras consiguiendo nuevas aplicaciones lineales. Estas operaciones son de gran importancia y se utilizan a lo largo de todo el texto.

Supongamos que \(F:V \rightarrow U\) y \(G:V \rightarrow U\) son aplicaciones lineales entre espacios vectoriales sobre un cuerpo \(K\). Definimos la suma \(F+G\) como la aplicación de \(V\) en \(U\) que asigna \(F(v) + G(v)\) a \(v\in V\):
\[
  (F+G)(v) = F(v) + G (v)
\]
Asimismo, para cualquier escalar \(k \in K\), definimos el producto \(kF\) como la aplicación en \(U\) que asigna \(kF(v)\) a \(v \in V\):
\[
  (kF)(v) = kF (v)
\]
Probamos ahora que si \(F\) y \(G\) son lineales, también lo son \(F + G\) y \(kF\). Tenemos, para todo par de vectores \(v,w \in V\) y todo par de escalares \(a,b \in K\).
\begin{align*}
  (F+G)(av + bw) &= F(av + bw) + G(av + bw) = \\
                 &=aF(v) + bF(w) + aG(v) + bG(w) = \\
                 &=a(F(v) + G(v)) + b(F(w) + G(w)) = \\
                 &=a(F+G)(v) + b(F+G)(w) =\\[3pt]
  (kF)(av + bw) &= kF(av + bw) = k(aF(v)+bF(w))= \\
                &= akF(v) + bkF(w) = a(kF)(v) + b(kF)(w)
\end{align*}
Así pues \(F+G\) y \(kF\) son lineales.

Disponemos del siguiente teorema:
\teorema Sean \(V\) y \(U\) espacios vectoriales sobre un cuerpo \(K\). Ka colección de todas las aplicaciones lineales de \(V\) en \(U\), con las operaciones de suma y producto por un escalar anteriores, es un espacio vectorial sobre \(K\).

El espacio vectorial del teorema suele denotarse por:
\[
\text{Hom}(V,U)
\]
Aquí \(\text{Hom}\) viene de la palabra homomorfismo. En caso de ser \(V\) y \(U\) de dimensión finita, se aplica el teorema que enseguida enunciamos,

Supongamos \(\dim V = m\) y \(\dim U=n\). En tal caso, \(\dim(\text{Hom}(V,U)) = m\cdot n\).

\subsection{Composición de aplicaciones lineales}

Sean \(U, V\) y \(W\) espacios vectoriales sobre un mismo cuerpo \(K\) y \(F : V \rightarrow U\) y \(G: U \rightarrow W\) aplicaciones lineales:
\begin{center}
  \begin{tikzpicture}[
    node distance=3cm,
    every node/.style={font=\large},
    set/.style={circle, draw, minimum size=1cm, align=center},
    arrow/.style={-Stealth, thick}
    ]

    \node[set] (V) {\(V\)};
    \node[set, right=of V] (U) {\(U\)};
    \node[set, right=of U] (W) {\(W\)};

    \draw[arrow] (V) -- node[above] {\(F\)} (U);
    \draw[arrow] (U) -- node[above] {\(G\)} (W);
    \draw[arrow] (V) -- node[above] {\(f\)} (U);

  \end{tikzpicture}
\end{center}

Recordemos que la función compuesta \(G \circ F\) es la aplicación de \(V\) en \(W\) definida por \((G \circ F) (v) = G(F(v))\). Probemos que \(G\circ F\) es lineal siempre que lo sean \(F\) y \(G\). Tenemos, para todo par de vectores \(v,w \in V\) y todo par de escalares \(a,b \in K\),

\begin{align*}
  (G \circ F)(av + bw) &= G(F(av+bw)) = G(aF(v)+bF(w)) =\\
  &=aG(F(v)) + bG(F(w)) = a(G \circ F)(v) + b(G \circ F)(w)  
\end{align*}

Es decir \(G \circ F\) es lineal.

La composición de aplicaciones lineales está relacionada con la suma y el producto por un escalar como sigue:

Sean \(V, U\) y \(W\) espacios vectoriales sobre \(K\), \(F\) y \(F'\) aplicaciones lineales de \(V\) en \(U\), \(G\) y \(G'\) aplicaciones lineales de \(U\) en \(W\) y sea \(k \in K\). Entonces:
\begin{align*}
  G \circ (F+F') = G\circ F + G \circ F'\\
  (G+G') \circ F = G \circ F + G' \circ F \\
  k(G\circ F) = (kG) \circ F = G \circ (kF)
\end{align*}

\subsection{Algebra de operadores lineales \(A(V)\)}

Sea \(V\) un espacio vectorial sobre un cuerpo \(K\). Consideramos ahora el caso especial de las aplicaciones lineales \(F:V\rightarrow V\), o sea, de \(V\) en si mismo. Estas se llaman también operadores lineales o transformaciones lineales en \(V\). Escribiremos \(A(V)\), en lugar de \(\text{Hom}(V,V)\), para designar el espacio de tales aplicaciones.

De acuerdo con el teorema (9.10??), \(A(V)\) es un espacio vectorial sobre \(K\); es de dimensión \(n^2\) si \(V\) es de dimensión \(n\). Si \(F,G \in A(V)\), la composición \(G \circ F\) existe y es también una aplicación lineal de \(V\) en sí mismo, es decir, \(G\circ F \in A(V)\). De este modo, tenemos un <<producto>> definido en \(A(V)\).[En el espacio \(A(V)\) expresaremos \(G \circ F\) como \(GF\).]

\textbf{Nota}: Un álgebra \(A\) sobre un cuerpo \(K\) es un espacio vectorial sobre \(K\) en el que se ha definido una operación de producto que satisface, para todos los \(F,G,H \in A\) y todo \(k \in K\),
\begin{enumerate}
  \item \(F(G+H)=FG+FH\)
  \item \((G+H)F = GF+HF\)
  \item \(k(GF)=(kG)F = G(kF)\)
\end{enumerate}
Si la ley asociativa rige para el producto, esto es, si para todos los \(F,G,H \in A\), \((FG)H = F(GH)\) se dice que el álgebra \(A\) es asociativa.

Enunciamos el siguiente teorema:

Sea \(V\) un espacio vectorial sobre \(K\). En ese caso, \(A(V)\) es un álgebra asociativa sobre \(K\) con respecto a la composición de aplicaciones. Si \(\dim(V) = n\), \(\dim(A(V))=n^2\).

En vista del teorema anterior, \(A(V)\) se denomina frecuentemente el álgebra de operadores lineales en \(V\).

\subsection{Polinomios y operadores lineales}

Observemos que la aplicación identidad \(I:V\rightarrow V\) pertenece a \(A(V)\). Además para todo \(T\in A(V)\), tenemos \(T\cdot I = I \cdot T = T\). Hacemos notar que también pueden formarse <<potencias>> de \(T\); usamos la notación \(T^2=T \circ T\), \(T^3=T\circ T \circ T\), \(\cdots\). Más aún, para todo polinomio
\[
  p(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n \quad a_i \in K
\]
podemos construir el operador \(p(T)\) definido según
\[
  p(T) = a_0 I + a_1 T + a_2 T^2 + \cdots + a_n T^n
\]
(Para un escalar \(k \in K\), el operador \(kI\) se denota con frecuencia por \(k\), simplemente.) En particular, si \(p(T)=0\), la aplicación cero, se dice que \(T\) es un \textit{cero} del polinomio \(p(x)\).

\ejemplo{ Definamos \(T:\mathbb{R}^3 \rightarrow \mathbb{R}^3\) por \(T(x,y,z)=(0,x,y)\). Si \(a,b,c\) es cualquier elemento de \(\mathbb{R}^3\).}
\begin{align*}
  (T+I)(a,b,c) = (0,a,b) + (a,b,c) = (a,a+b,b+c) \\
  T^3(a,b,c) = T^2(0,a,b) = T(0,0,a) = (0,0,0)
\end{align*}
Vemos, pues, que \(T^3 = 0\), la aplicación cero de \(V\) en sí mismo. Dicho de otro modo, \(T\) es un cero del polinomio \(p(x)=x^3\).

\subsection{Operadores invertibles}

Decimos que un operador lineal \(T:V\rightarrow V\) es \textit{invertible} si tiene una inversa, es decir, si existe \(T^{-1}\in A(V)\) tal que \(TT^{-1} = T^{-1}T = 1\).

Ahora bien, \(T\) es invertible si y sólo si es inyectivo y suprayectivo. De este modo, en particular, si \(T\) es invertible, solamente \(0\in V\) puede aplicarse en si  mismo, esto es, \(T\) es no singular. El recíproco no es cierto en general como se ve en el siguiente ejemplo:

\ejemplo

Sean \(V\) el espacio vectorial de los polinomios sobre \(K\) y \(T\) el operador en \(V\) definido según:
\[
  T(a_0 + a_1t + \cdots + a_n t^n) = a_0t + a_1 t^2 + \cdots + a_n t^{n+1} \qquad (n=0,1,2,\cdots)
\]
o sea, \(T\) aumenta en una unidad el exponente de \(t\) en cada término. Tenemos que \(T\) es una aplicación lineal y es no singular. Sin embargo, \(T\) no es suprayectivo y por lo tanto no es invertible.

La situación cambia significativamente cuando \(V\) es de dimensión finita. En concreto disponemos del teorema enunciado a continuación.

\teorema{ Supongamos que \(T\) es un operador lineal en un espacio vectorial \(V\) de dimensión finita. Son equivalentes las cuatro condiciones:}
\begin{enumerate}
  \item \(T\) es no singular, es decir \(\text{Ker }T = \{0\}\).
  \item \(T\) es inyectivo.
  \item \(T\) es suprayectivo.
  \item \(T\) es invertible, esto es, inyectivo y suprayectivo.
\end{enumerate}

Ya probamos que 1) y 2) son equivalentes. Siendo así para demostrar el teorema, solo necesitamos probar que lo son 1) y 3). Se desprende entonces que 4) es equivalente de las otras.

Según vimos:
\[
  \dim V = \dim(\text{Im } T) + \dim(\text{Ker } T)
\]
Si \(T\) es no singular, necesariamente \(\dim(\text{Ker } T) = 0\), con lo cual, \(\dim V = \dim(\text{Im }T)\). Esto significa que \(V = \text{Im } T\) o que \(T\) es suprayectivo, de manera que 1) implica 3). Recíprocamente supongamos que \(T\) es suprayectivo. En ese caso, \(V = \text{Im }T\), de donde \(\dim V = \dim(\text{Im }T)\). Esto quiere decir que \(\dim(\text{Ker } T) = 0\) y por ende \(T\) es no singular. Así pues, 3) implica 1). En consecuencia, el teorema queda demostrado. (La demostración del teorema (9.9??) es idéntica).

\ejemplo{ Sea \(T\) el operador en \(\mathbb{R}^2\) definido por \(T(x,y)=(y,2x-y)\). El núcleo de \(T\) es \(\{(0,0)\}\) luego \(T\) es no singular y, en virtud del Teorema,}

Hallemos ahora una fórmula para \(T^{-1}\).

Supongamos que \(s,t\) es la imagen de \((x,y)\) bajo \(T\); por consiguiente \((x,y)\) es la imagen de \((s,t)\) bajo \(T^{-1}:T(x,y) = (s,t)\) y \(T^{-1}(s,t)=(x,y)\). Tenemos:
\[
  T(x,y) = (y,2x-y) = (s,t) \quad \text{y así} \quad y=s,2x-y = t
\]
Despejando \(x\) e \(y\) en términos de \(s\) y \(t\) obtenemos \(x=\frac{1}{2}s + \frac{1}{2}t,~ y=s\). Entonces \(T^{-1}\) viene dado por la fórmula \(T^{-1}(s,t) = \left(\frac{1}{2}s+\frac{1}{2}t,~s\right)\).

\subsection{Aplicaciones a los sistemas de ecuaciones lineales}

Consideremos un sistema de ecuaciones lineales sobre \(K\) y supongamos que el sistema tiene el mismo número de ecuaciones que de incógnitas, digamos \(n\). Podemos representar este sistema por la ecuación matricial
\begin{equation}
  Ax = b
  \label{eq:sistema_lineal_5}  
\end{equation}
donde \(A\) es la matriz n-cuadrada sobre \(K\) que vemos como un operador lineal en \(K^n\). Supongamos que la matriz \(A\) es no singular, esto es, que la ecuación matricial \(Ax = 0\) tiene sólo la solución trivial. La aplicación lineal \(A\) es, pues, inyectiva y suprayectiva. Esto significa que el sistema \ref{eq:sistema_lineal_5} tiene solución única para cualquier \(b \in K^n\). Por otra parte, supongamos que la matriz \(A\) es singular, es decir, que la ecuación matricial \(Ax = 0\) tiene una solución no nula. En ese caso la aplicación lineal \(A\) no es suprayectiva, lo que significa que existe \(b \in K^n\) para el que \ref{eq:sistema_lineal_5} no tiene solución. Más aún, si existe una solución, esta no es única.

\teorema{Consideremos un sistema de ecuaciones lineales con igual número de incógnitas y de ecuaciones}
\begin{enumerate}[label=\alph*.]
  \item Si el sistema homogéneo asociado sólo tiene la solución nula, el sistema precedente tiene solución única para todos los valores de los \(b_i\).
  \item Si el sistema homogéneo asociado tiene una solución no nula, entonces: i) hay valores de los \(b_i\) para los que el sistema precedente no tiene solución; ii) siempre que exista una solución del sistema anterior, no será única.
\end{enumerate}

\subsubsection{Matriz asociada a una función lineal}

Se tiene una función \(f:V \rightarrow W\) ambos espacios vectoriales sobre el mismo cuerpo \(K\) \(B=\left\{u_1,u_2,\cdots,u_n \right\}\) es una base de \(V\), \(B'={u'_1, u'_2, \cdots, u'_m}\) es una base de \(W\), si se conoce:
\[
f(u_1) = a_{11} u'_1 + a_{21} u'_2 + \cdots + a_{m1} u'_m
 = \begin{pmatrix}
  a_{11} \\
  a_{21} \\
  \vdots \\
  a_{m1}
 \end{pmatrix}
\]
\[
f(u_2) = a_{12} u'_1 + a_{22} u'_2 + \cdots + a_{m2} u'_m
 = \begin{pmatrix}
  a_{12} \\
  a_{22} \\
  \vdots \\
  a_{m2}
 \end{pmatrix}
\]
\[
\vdots
\]
\[
f(u_n) = a_{1n} u'_1 + a_{2n} u'_2 + \cdots + a_{mn} u'_m
 = \begin{pmatrix}
  a_{1n} \\
  a_{2n} \\
  \vdots \\
  a_{mn}
 \end{pmatrix}
\]
Entonces la imagen de un vector \(u\) de \(V\) es un vector \(w\) de \(W:f(u) = w\)
\begin{align*}
  f(u) &= \alpha_1 f(u_1) + \alpha_2 f(u_2) + \cdots + \alpha_n f(u_n) \\
  &= \alpha_1 \begin{pmatrix}
    a_{11} \\ a_{21} \\ \vdots \\ a_{m1}
  \end{pmatrix} + \alpha_2 \begin{pmatrix}
    a_{12} \\ a_{22} \\ \vdots \\ a_{m2}
  \end{pmatrix} + \cdots + \alpha_n \begin{pmatrix}
    a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}
  \end{pmatrix} = \begin{pmatrix}
    \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m
  \end{pmatrix} \\[4pt]
  &= \begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    a_{21} & a_{22} & \cdots & a_{2n}\\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}\\
  \end{pmatrix} \cdot \begin{pmatrix}
    \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n
  \end{pmatrix} = \begin{pmatrix}
    \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m
  \end{pmatrix} = w
\end{align*}
La matriz \(A = (a_{ij})\) que contiene a las imágenes de los vectores básicos de \(B\) expresados en \(B'\) se llama matriz asociada a la función lineal \(f\) en dichas bases.

\ejemplo{ Si se trabaja con las bases canónicas en dominio y codiminio}
\[
f:\mathbb{R}^2 \rightarrow \mathbb{R}^2 \quad \text{tal que} \quad f\begin{pmatrix}
  x \\ y \\ z
\end{pmatrix} = \begin{pmatrix}
  x+y \\ y-z
\end{pmatrix}
\]
Se halla la imagen de los vectores de la base canónica de \(\mathbb{R}^3\):
\[
f\begin{pmatrix}
  1 \\ 0 \\ 0 
\end{pmatrix} = \begin{pmatrix}
  1 \\ 0
\end{pmatrix} \qquad f\begin{pmatrix}
  0 \\ 1 \\ 0
\end{pmatrix} = \begin{pmatrix}
  1 \\ 1
\end{pmatrix} \qquad f \begin{pmatrix}
  0 \\ 0 \\ 1
\end{pmatrix} = \begin{pmatrix}
  0 \\ -1
\end{pmatrix}
\]
Luego, la matriz asociada \(A\) a esta función en las bases canónicas resulta:
\[
A = \begin{pmatrix}
  1 & 1 & 0 \\
  0 & 1 & -1
\end{pmatrix}
\] 
A partir de ella es posible hallar la imagen de algún vector del dominio:
\[
u = \begin{pmatrix}
  2 \\ 1 \\ -1
\end{pmatrix} \quad \text{por ejemplo}
\]
entonces hacemos \(A\cdot u = f(u)\):
\[
\begin{pmatrix}
  1 & 1 & 0 \\
  0 & 1 & -1
\end{pmatrix} \cdot \begin{pmatrix}
  2 \\ 1 \\ -1
\end{pmatrix} = \begin{pmatrix}
  3 \\ 2
\end{pmatrix}
\]

\ejemplo{ Cuando en el dominio y codominio se conocen bases distintas canónicas}

En \(\mathbb{R}^2\) se tiene la base \(B = \left\{(-1,1),~(1,2)\right\}\) y en \(\mathbb{R}^3\) se tiene \(B'=\left\{(1,1,1),~(1,1,0),~(-1,0,0)\right\}\)
\[
f:\mathbb{R}^2 \rightarrow \mathbb{R}^3 \quad \text{dada por}\quad f\begin{pmatrix}
  x \\ y
\end{pmatrix} = \begin{pmatrix}
  x+y \\ x-y \\ y
\end{pmatrix}
\]
Hallamos la imagen de los vectores de \(B\) y los expresamos en la base \(B'\):
\[
f\begin{pmatrix}
  -1 \\ 1
\end{pmatrix} = \begin{pmatrix}
  0 \\ -2 \\ 1
\end{pmatrix} = \alpha_1 \begin{pmatrix}
  1 \\ 1 \\ 1
\end{pmatrix} + \alpha_2 \begin{pmatrix}
  1 \\ 1 \\ 0
\end{pmatrix} + \alpha_3 \begin{pmatrix}
  -1 \\ 0 \\ 0
\end{pmatrix} = \begin{pmatrix}
  1 \\ -3 \\ -2
\end{pmatrix}
\]
\[
f\begin{pmatrix}
  1 \\ 2
\end{pmatrix} = \begin{pmatrix}
  3 \\ -1 \\ 2
\end{pmatrix} = \alpha_1 \begin{pmatrix}
  1 \\ 1 \\ 1
\end{pmatrix} + \alpha_2 \begin{pmatrix}
  1 \\ 1 \\ 0
\end{pmatrix} + \alpha_3 \begin{pmatrix}
  -1 \\ 0 \\ 0
\end{pmatrix} = \begin{pmatrix}
  2 \\ -3 \\ -4
\end{pmatrix}
\]
\begin{tcolorbox}[mydanger]
  \centering
  REVISAR, acá hay algo mal, falta aclarar bien qué está en que base. No es todo igualdades!
\end{tcolorbox}

Por lo tanto la matriz asociada \(A\) asociada a \(f\) en las bases \(B\) y \(B'\) es:
\[
A = \begin{pmatrix}
  1 & 2 \\ -3 & -3 \\ -2 & -4
\end{pmatrix}
\] 

Si queremos hallar a partir de ella la imagen de un vector cualquiera de \(\mathbb{R}^2\):
\[
u = \begin{pmatrix}
  2 \\ -3
\end{pmatrix} \quad \text{por ejemplo,}
\]
basta con hacer \(A \cdot u = f(u)\):
\[
\begin{pmatrix}
  1 & 2 \\ -3 & -3 \\ -2 & -4
\end{pmatrix} \cdot \begin{pmatrix}
  2 & -3
\end{pmatrix} = \begin{pmatrix}
  -4 \\ 3 \\ 8
\end{pmatrix}
\]
Esta es la imagen de \(u\) en las bases de \(B\) y \(B'\) dada en \(\mathbb{R}^2\) y \(\mathbb{R}^3\) respectivamente.

De no disponer de la matriz \(A\) asociada a la función en las bases \(B\) y \(B'\), habría que determinar a la función \(f\) en la base \(B\), luego calcular con ella la imagen del vector \(u\) y a esa imagen obtenida expresarla en la base \(B'\) del codominio.

\begin{tcolorbox}
  La matriz asociada a una función lineal tiene \(m\) filas y \(n\) columnas, el número de filas lo determina la dimensión del codominio y el número de columnas la dimensión del dominio.
\end{tcolorbox}