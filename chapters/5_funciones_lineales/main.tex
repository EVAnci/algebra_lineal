\section{Funciones Lineales}

Una función lineal (también llamada transformación lineal o aplicación lineal) es una función entre dos espacios vectoriales que preserva la estructura algebraica de los vectores, es decir, respeta la suma de vectores y la multiplicación por escalares.

\textbf{Definición}: Sea \(f: V \to U\) una función entre espacios vectoriales sobre el mismo cuerpo \(K\). 
\begin{align*}
  f: V &\rightarrow U\\
  v &\rightarrow u
\end{align*}
que a cada vector \(v\) de \(V\) le hace corresponder un único vector \(f(v)\) de \(U\).

Diremos que \(f\) es una aplicación lineal si para todo \(v, w \in V\) y todo escalar \(t \in K\), se cumple:
\begin{itemize}
  \item \(f(v + w) = f(v) + f(w)\)
  \item \(f(t \cdot v) = t \cdot f(v)\)
\end{itemize}

La primera condición se la conoce como \textit{aditividad}, indica que la función respeta las operaciones internas definidas en los espacios vectoriales involucrados, la segunda se denomina \textit{homogeneidad} y está indicando que la función también respeta las operaciones externas. 

\begin{tcolorbox}[remember, title=Aclaración]
  \(V(K)\) y \(U(K)\) son dos espacios vectoriales definidos sobre un mismo cuerpo \(K\) (por ejemplo, \(\mathbb{R}\) o \(\mathbb{C}\)).
  
  La función \(f: V \rightarrow U\) toma un vector \(v \in V\) y lo lleva a un vector \(f(v) \in U\).
  
  Es importante entender que tanto el dominio como el codominio son espacios vectoriales, por lo tanto, en ambos hay operaciones de suma y producto por escalar.
\end{tcolorbox}

En otras palabras, una función \(f: V \rightarrow U\) es lineal si cumple dos condiciones fundamentales para todo \(v, w \in V\) y \(t \in K\):
\begin{enumerate}
  \item Aditividad (respeta la suma):
    \[
     f(v + w) = f(v) + f(w)
    \]
  \item Homogeneidad (respeta el producto por escalares):
    \[
      f(t \cdot v) = t \cdot f(v)
    \]
\end{enumerate}

Estas dos propiedades garantizan que la función ``conserva'' la estructura lineal. En otras palabras, las combinaciones lineales en el espacio de partida se transforman en combinaciones lineales en el espacio de llegada, de la misma forma.

\paragraph{Propiedades de una función lineal}

Si \(f\) es una función lineal de \(V(K)\) en \(U(K)\) entonces:
\begin{enumerate}
  \item \(f\left(\vec{0}\right) = \vec{0}, \qquad \left(\vec{0} \in V,\vec{0} \in U\right)\)

  Demostración:\[
    f\left(\vec{0}\right) = f\left(0 \cdot v\right) = 0 \cdot f(v) = \vec{0}
  \]
  \item \(f(-v)=-f(v), \quad \forall v \in V\)
  
  Demostración: \[
    f(-v) = f(-1 \cdot v) = -1 \cdot f(v) = -f(v)
  \]
  \item \(f(v-w)=f(v) - f(w) \quad \forall v,w \in V\)
  
  Demostración: \[
    f(v-w) = f(v) + f(-w) = f(v) + (-f(w)) = f(v) - f(w)
  \]
\end{enumerate}

\begin{tcolorbox}[interesting_data, title=Nota conceptual]
  Las propiedades desarrolladas son sumamente importantes. Por ejemplo la primer propiedad distingue claramente las funciones lineales de otras funciones. Por ejemplo, si una función \(f\) no cumple que \(f\left(\vec{0}\right) = \vec{0}\), entonces automáticamente no es lineal.
\end{tcolorbox}

Ahora para todo par de escalares \(a,b \in K\) y para todo par de vectores \(v,w \in V\), obtenemos imponiendo ambas condiciones de linealidad:
\[
  F(av + bw) = a\cdot f(v) + b\cdot f(w) \qquad \text{se utiliza para definirlas}
\]
Con mayor generalidad, para escalares \(a_i \in K\) y vectores \(v_i \in V\), llegamos:
\[
  F(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n) = a_1 f(v_1) + a_2 f(v_2) + \cdots + a_n f(v_n)
\]
\begin{tcolorbox}[myconclusion]
  Expresión utilizada para demostrar los teoremas.
\end{tcolorbox}

\begin{quote}
  \ejemplo{ Veamos algunos ejemplos}
  \begin{enumerate}[label=\alph*.]
    \item Sea \(A\) cualquier matriz \(m \times n\) sobre un cuerpo \(K\). Como se señaló previamente, \(A\) determina una aplicación \(F:K^n \rightarrow K^m\) mediante la asignación \(v \rightarrow Av\) (aquí los vectores de \(K^n\) y \(K^m\) se escriben como columnas). Afirmamos que \(F\) es lineal. Esto es debido a que, por propiedades de las matrices:
    \begin{align*}
      F(v + w) &= A(v+w) = Av + Aw = F(v) + F(w) \\
      F(kv) &= A(kv) = kAv = kF(v)
    \end{align*}
    donde \(v, w \in K^n\) y \(k \in K\).
    \item Sea \(F:\mathbb{R}^3 \rightarrow \mathbb{R}^3\) la aplicación <<proyección>> en el plano \(xy: F(x,y,z)= (x,y,0)\). Probemos que \(F\) es lineal. Sean \(v=(a,b,c)\) y \(w=(a',b',c')\). Entonces:
    \begin{align*}
      F(v+w) &= F(a+a', b+b', c+c') = (a+a', b+b',0) = \\
            &= (a,b,0) + (a',b',0) = F(v) + F(w)
    \end{align*}
    y para todo \(k \in \mathbb{R}\):
    \[
      F(kv) = F(ka,kb,kc) = (ka,kb,0) = k(a,b,0) = kF(v)
    \]
    O sea, \(F\) es lineal.
    \item Sea \(F: \mathbb{R}^2 \rightarrow \mathbb{R}^2\) la aplicación de <<traslación>> definida según \(F(x,y) = (x+1,y+2)\). Obsérvese que \(F(0)=(0,0)=(1,2)\neq 0\). Es decir, el vector cero no se aplica sobre el vector cero. Por consiguiente \(F\) no es lineal.
    \item Sea \(F:V\rightarrow U\) la aplicación que asigna \(0 \in U\) a todo \(v \in V\). Para todo par ded vectores \(v,w \in V\) y todo \(k \in K\) tenemos:
    \[
      F(v+w) = 0 = 0+0 = F(v) + F(w) \qquad \text{y} \qquad F(kv) = 0 = k0 = kF(v)
    \]
    Así \(F\) es lineal. Llamamos a \(F\) la \textit{aplicación cero} y la denotaremos normalmente por \(0\).
    \item Consideremos la aplicación identidad \(I:V\rightarrow V\) que aplica cada \(v\in V\) en si mismo. Para todo par de vectores \(v, w \in V\) y todo par de escalares \(a,b \in K\):
    \[
      I(av+bw)= av + bw = aI(v) + bI(w)
    \]
    Así \(I\) es lineal.
    \item Sea \(V\) el espacio vectorial de los polinomios en la variable \(t\) sobre el cuerpo real \(\mathbb{R}\). La aplicación derivada \(\mathbf{D}:V\rightarrow V\) y la aplicación integral \(\mathbf{J}:V\rightarrow \mathbb{R}\), son lineales. La razón es que, según se demuestra en el cálculo, para todo par de vectores \(u,v \in V\) y todo \(k\in \mathbb{R}\):
    \[
      \frac{d(u+v)}{dt} = \frac{du}{dt} + \frac{dv}{dt} \qquad \text{y} \qquad \frac{ku}{dt} = k\frac{du}{dt}
    \]
  \end{enumerate}
 \end{quote}

\subsubsection{Clasificación de las funciones lineales}

Se tiene una función lineal \(f\) de \(V(K)\) en \(W(K)\):
\begin{itemize}
  \item Si \(f\) es inyectiva recibe el nombre de \textit{monomorfismo}
  \item Si \(f\) es suryectiva recibe el nombre de \textit{epimorfismo}
  \item Si \(f\) es biyectiva recibe el nombre de \textit{isomorfismo}
  \item Si en \(f \quad V=W\) recibe el nombre de \textit{endomorfismo}
  \item Si \(f\) es un endomorfismo y es biyectiva recibe el nombre de \textit{automorfismo} 
\end{itemize}

\subsubsection{Núcleo e imagen de una función lineal}

Sea \(f\) una función lineal de \(V(K)\) en \(U(K)\) \(\left(f:V\rightarrow U\right)\), asociados a esta función existen dos subconjuntos, uno del conjunto de partida y otro del conjunto de llegada, que reciben el nombre de \textit{núcleo} (o \textit{Ker}) de la función e imagen de la función, los definimos:
\[
\text{Ker } f = N(f) = \left\{ v \in V \mid f(v) = \vec{0}_U \right\}
\]
\begin{center}
  son todos los vectores de \(V\) que tienen como imagen al vector nulo de \(U\)
\end{center}
\[
\text{Im } f = \left\{u\in U \mid \exists v \in V ~\text{ tal que }~ f(v) = u \right\}
\]
o también en una forma más abreviada:
\[
\text{Im } f = \left\{f(v) \mid v \in V\right\}
\]
\begin{center}
  son todos los vectores de \(U\) que son imagen de algún vector \(v \in V\)
\end{center}

Los subconjuntos \(N(f)\) e \(\text{Im } f\) son subespacios de \(V\) y de \(U\) respectivamente.

\begin{quote}
  \ejemplo{ Consideremos la siguiente función lineal:}
  \[
    f(x) = 3x
  \]
  Esta es una función lineal \(f:\mathbb{R} \rightarrow \mathbb{R}\).
  \begin{itemize}
    \item \textbf{Núcleo:}
    \[
      \text{Ker } f = \left\{x \in \mathbb{R} \mid f(x) = 0\right\} = \{0\}
    \]
    \item \textbf{Imagen:}
    \[
      \text{Im } f = \left\{f(x) \mid x \in \mathbb{R}\right\} = \left\{3x \mid x \in \mathbb{R}\right\} = \mathbb{R}
    \]
  \end{itemize}
  Aquí el \textit{único} elemento que se anula es el \(0\), pero la imagen es \textit{todo} \(\mathbb{R}\).

  \vspace{5mm}

  \ejemplo{ Ahora consideremos la función trivial de \(\mathbb{R}\) a \(\mathbb{R}\):}
  \[
    f(x) = 0
  \]
  Esta función también es lineal, y \(f:\mathbb{R} \rightarrow \mathbb{R}\)
  \begin{itemize}
    \item \textbf{Núcleo:}
    \[
      \text{Ker } f = \left\{x \in \mathbb{R} \mid f(x) = 0\right\} = \mathbb{R}
    \]
    \item \textbf{Imagen:}
    \[
      \text{Im } f = \{0\}
    \]
  \end{itemize}
  En este caso, \textit{todo} el dominio se anula y la imagen es el conjunto reducido que contiene solo al cero.
\end{quote}

\begin{tcolorbox}[title=Resumen para fijar la idea]
  \begin{itemize}
    \item El \textbf{núcleo} es un subconjunto del \textbf{dominio} \(V\): son los vectores que van a parar al cero del codominio.
    \item La \textbf{imagen} es un subconjunto del codominio \(U\): son todos los valores posibles que puede tomar \(f(v)\)
  \end{itemize}

  \vspace{5mm}

  El núcleo responde a la pregunta: ``¿Qué vectores se anulan bajo \(f\)?''

  La imagen responde a: ``¿Qué vectores pueden obtenerse como salida de \(f\)''
\end{tcolorbox}

\subsubsection{Los subespacios Núcleo y Imagen de \(f\)}

\paragraph{El conjunto núcleo de \(f\) es un subespacio del dominio}

\textbf{Proposición}: Sea \(f: V \to U\) una aplicación lineal entre espacios vectoriales. Entonces, el conjunto núcleo de \(f\),
\[
\text{Ker } f = \{ v \in V \mid f(v) = 0 \}
\]
es un subespacio vectorial de \(V\).

\textit{Demostración}
\begin{quote}
  \begin{enumerate}
    \item \textbf{No vacío}:
    
    El vector nulo del dominio \(\vec{0}_V \in V\) cumple que:
      \[
      f(\vec{0}_V) = \vec{0}_U
      \]
    por lo tanto, \(\vec{0}_V \in \text{Ker } f\), y así \(\text{Ker } f \ne \emptyset\).
  
    \item \textbf{Cerrado bajo la suma}:
    
      Sean \(v_1, v_2 \in \text{Ker } f\). Entonces:
     \[
     f(v_1) = 0 \quad \text{y} \quad f(v_2) = 0
     \]
     Como \(f\) es lineal:
     \[
     f(v_1 + v_2) = f(v_1) + f(v_2) = 0 + 0 = 0
     \]
     Entonces \(v_1 + v_2 \in \text{Ker } f\).
  
    \item \textbf{Cerrado bajo el producto por escalares}:
     
      Sea \(t \in K\) y \(v \in \text{Ker } f\), es decir, \(f(v) = 0\). Entonces:
     \[
     f(t \cdot v) = t \cdot f(v) = t \cdot 0 = 0
     \]
     Por lo tanto, \(t \cdot v \in \text{Ker } f\).
  \end{enumerate}
  
  Conclusión: Se cumplen las tres condiciones necesarias para que \(\text{Ker } f\) sea un subespacio de \(V\).
  \(\blacksquare\)
\end{quote}

\paragraph{El conjunto imagen de \(f\) es un subespacio del codominio}


\textbf{Proposición}: Sea \(f: V \to U\) una aplicación lineal entre espacios vectoriales sobre un mismo cuerpo \(K\). Entonces el conjunto
\[
\text{Im } f = \{ u \in U \mid \exists v \in V \text{ tal que } f(v) = u \}
\]
es un subespacio vectorial de \(U\).

\textit{Demostración:}
\begin{quote}
  \begin{enumerate}
    \item \textbf{No vacío}:
      Como \(f\) es lineal, se cumple que:
      \[
      f(\vec{0}_V) = \vec{0}_U
      \]
      Entonces, \(\vec{0}_U \in \text{Im } f\), por lo tanto \(\text{Im } f \ne \emptyset\).

    \item \textbf{Cerrado bajo suma y producto por escalares} (en una sola propiedad):

      Sean \(u_1, u_2 \in \text{Im } f\).
      Por definición, existen \(v_1, v_2 \in V\) tales que:
      \[
      f(v_1) = u_1 \quad \text{y} \quad f(v_2) = u_2
      \]
      Sean \(a, b \in K\) escalares arbitrarios. Como \(f\) es lineal:
      \[
      f(a v_1 + b v_2) = a f(v_1) + b f(v_2) = a u_1 + b u_2
      \]
      Esto significa que \(a u_1 + b u_2 \in \text{Im } f\).
  \end{enumerate}

  Conclusión: La imagen de \(f\) cumple las condiciones necesarias para ser subespacio de \(U\): no es vacía, y es cerrada bajo combinaciones lineales.
  \(\blacksquare\)
\end{quote}

\begin{tcolorbox}[title=Observaciones]
  Note que no necesitamos comprobar ``cerrado bajo suma'' y ``cerrado bajo producto escalar'' por separado, ya que probar cerrado bajo combinaciones lineales es más general y suficiente.
\end{tcolorbox}


%%%% A partir de aquí he copiado todo tal y como sale en el apunte de artal. Luego lo modificaré.

Ejemplos
\label{ej:ejemplos_a_revisar}
\begin{enumerate}[label=\alph*.]
  \item Sea \(F:\mathbb{R}^3 \rightarrow \mathbb{R}^3\) la aplicación proyección sobre el plano \(xy\). Esto es,
  \[
    F(x,y,z)=(x,y,0)
  \]
  (Véase la figura {insertar figura aquí}) Claramente la imagen de \(F\) es todo el plano \(xy\). Es decir, 
  \[
    \text{Im } F = \left\{(a,b,c):c\in \mathbb{R}\right\}
  \]
  Nótese que el núcleo de \(F\) es el eje \(z\). O sea,
  \[
    \text{Ker } F = \left\{(0,0,c):c\in \mathbb{R}\right\}
  \]
  ya que estos puntos y solamente éstos se aplican en el vector cero \(0=(0,0,0)\).
  \item Sean \(V\) el espacio vectorial de los polinomios sobre \(\mathbb{R}\) y \(\mathbf{T}:V\rightarrow V\) el operador tercera derivada, esto es,
  \[
    \mathbf{T}\left[f(t)\right] = \frac{d^3 f}{dt^3}
  \]
  A veces se emplea la notación \(\mathbf{T}=\mathbf{D}^3\), donde \(\mathbf{D}\) es la aplicación derivada. Tendremos:
  \[
    \text{Ker } T = \left\{\text{polinomios de grado} \leq 2\right\}
  \]
  porque \(\mathbf{T}(at^2+bt+c)=0\) pero \(\mathbf{T}(t^n)\neq 0\) para \(n > 3\). Por otra parte
  \[
    \text{Im } T = V
  \]
  puesto que todo polinomio en \(V\) es la tercera derivada de algún polinomio.
\end{enumerate}

\paragraph{Teorema de la imagen de un sistema de generadores}

Supongamos ahora que los vectores \(v_1, \cdots, v_n\) generan \(V\) y que \(F:V\rightarrow U\) es lineal. Probemos que los vectores \(F(v_1),\cdots, F(v_n)\in U\) generan \(\text{Im } F\); en ese caso, \(F(v)=u\) para algún vector \(v\in V\). Como los \(v_i\) generan \(V\) y \(v \in V\), existen escalares \(a_1, \cdots, a_n\) tales que:
\[
  v = a_1 v_1 + a_2 v_2 + \cdots + a_n v_n
\]  
En consecuencia:
\[
  u = F(v) = F(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n) = a_1 F(v_1) + a_2 F(v_2) + \cdots + a_n F(v_n)
\]
y por lo tanto los vectores \(F(v_1), \cdots , F(v_n)\) generan \(\text{Im } F\).

Establezcamos formalmente este útil resultado.

Supongamos que \(v_1, v_2, \cdots, v_n\) generan un espacio vectorial \(V\) y que \(F:V\rightarrow U\) es lineal.

Entonces \(F(v_1), F(v_2), \cdots, F(v_n)\) generan \(\text{Im } F\).

\subsection{Rango y nulidad de una aplicación lineal}

Hasta aquí no hemos relacionado la noción de dimensión con la de aplicación lineal \(F : V \rightarrow U\). En pos casos enq ue \(V\) es de dimensión finita, tenemos la relación fundamental que sigue:

\teorema
Sea \(V\) de dimensión finita y sea \(F:V\rightarrow U\) una aplicación lineal. Entonces:
\begin{equation}
  \dim V = \dim( \text{Ker } F) + \dim(\text{Im } F)
  \label{eq:teorema_dimensión_nucleo_imagen}  
\end{equation}

El teorema proporciona, la siguiente fórmula para \(F\) cuando \(V\) tiene dimensión finita:

Es decir, la suma de las dimensiones de la imagen y el núcleo de una aplicación lineal es igual a la dimensión de su dominio. 

Se ve fácilmente que la ecuación (?9.1) se rige para la aplicación proyección de \(F\) del ejemplo (?9.5a - creo que es el primer ejemplo de \ref{ej:ejemplos_a_revisar}). Allí la imagen (plano \(xy\)) y el núcleo (eje \(z\)) de \(F\) tienen dimensiones 2 y 1, respectivamente, mientras que el dominio \(\mathbb{R}^3\) de \(F\) tiene dimensión 3.

\textbf{Nota}: Sea \(F:V\rightarrow U\) una aplicación lineal. Se define el \textit{rango} de \(F\) como la dimensión de su imagen y la \textit{nulidad} de \(F\) como la dimensión de su núcleo; esto es:
\[
  \text{rango } F = \dim(\text{Im } F) \qquad \text{y} \qquad \text{nulidad } F = \dim(\text{Ker } F)
\]
\[
\text{rango } F + \text{nulidad } F = \dim V
\]
Recordemos que el rango de una matriz se definió en origen como la dimensión de su espacio columna y de su espacio fila. Obsérvese que si ahora \(A\) como una aplicación lineal, las dos definiciones se corresponden, porque la imagen de \(A\) es precisamente su espacio columna.

Ejemplo

Sea \(F:\mathbb{R}^4 \rightarrow \mathbb{R}^3\) la aplicación lineal definida por:
\[
  F(x,y,s,t) = (x-y+s+t,x+2s-t,x+y+3s-3t)
\]
\begin{enumerate}[label=\alph*.]
  \item Encontramos una base y la dimensión de la imagen de \(F\).
  
  Hallamos la imagen de los vectores de la base usual de \(\mathbb{R}^4\):
  \begin{align*}
    F(1,0,0,0) = (1,1,1)\phantom{-} &\qquad F(0,0,1,0) = (1,2,3)\\
    F(0,1,0,0) = (-1,0,1) &\qquad F(0,0,0,1) = (1,-1,-3)
  \end{align*}
  Según la proposición (??) (revisar), los vectores imagen generan \(\text{Im } F\); por eso construimos la matriz cuyas filas son estos vectores imagen y la reducimos por filas a forma escalonada:
  \[
  \begin{pmatrix}
    1 & 1 & 1 \\
    -1 & 0 & 1 \\
    1 & 2 & 3 \\
    1 & -1 & -3
  \end{pmatrix} \sim \begin{bmatrix}
    1 & 1 & 1 \\
    0 & 1 & 2 \\
    0 & 1 & 2 \\
    0 & -2 & -4
  \end{bmatrix} \sim \begin{bmatrix}
    1 & 1 & 1 \\
    0 & 1 & 2 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
  \]
  De este modo, \((1,1,1)\) y \((0,1,2)\) constituyen una base de \(\text{Im } F\), luego \(\dim(\text{Im } F) = 2\) o, en otras palabras, rango \(F=2\).
  \item Encontraremos una base y la dimensión del núcleo de la aplicación \(F\).

  Hacemos \(F(v)=0\), donde \(v=(x,y,z,t)\):
  \[
    F(x,y,s,t) = (x-y+s+t, x+2s-t, x+y+3s-3t) = (0,0,0)
  \]
  Igualamos entre si las componentes correspondientes para formar el siguiente sistema homogéneo cuyo espacio solución es \(\text{Ker } F\):
  \begin{align*}
    \begin{cases}
      x - y + \phantom{1}s + \phantom{1}t = 0 \\
      x \phantom{+ 1y } + 2s - \phantom{1}t = 0 \\
      x + y + 3s - 3t = 0
    \end{cases} \quad \text{o} \quad \begin{cases}
      x - y + \phantom{1}s + \phantom{1}t = 0 \\
      \phantom{x +} \phantom{1}y + \phantom{1}s - 2t = 0 \\
      \phantom{x +} 2y + 2s - 4t = 0
    \end{cases} \quad \text{o} \quad \begin{cases}
      x-y+s+\phantom{1}t=0\\
      \phantom{x+} y+s-2t = 0
    \end{cases}
  \end{align*}
  Las variables libres son \(s\) t \(t\), luego \(\dim(\text{Ker } F)=2\) o \(\text{nulidad } F =2\). Tomamos:
  \begin{itemize}
    \item \(s=-1,t=0\), para obtener la solución \((2,1,-1,0)\).
    \item \(s=0,t=1\), para obtener la solución \((1,2,0,1)\).
  \end{itemize}
  Así \((2,1,-1,0)\) y \(1,2,0,1\) constituyen una base de \(\text{Ker } F\). (Obsérvese que \(\text{rango } F + \text{nulidad } F = 2 + 2 = 4\)), que es la dimensión del dominio \(\mathbb{R}^4\) de \(F\).
\end{enumerate}

\subsection{Aplicación a los sistemas de ecuaciones lineales}

Consideremos un sistema de \(m\) ecuaciones lineales con \(n\) incógnitas sobre un cuerpo \(K\):
\begin{align*}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n &= b_1 \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n &= b_2\\
  \vdots  \qquad \qquad ~ & \\
  a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n &= b_m
\end{align*}
que es equivalente a la ecuación matricial
\[
  Ax=b
\]
donde \(A=(a_{ij})\) es la matriz de los coeficientes y \(x=(x_i)\) y \(b=(b_i)\) los vectores columna de las incógnitas y de las constantes, respectivamente. Ahora la matriz \(A\) puede verse también como la aplicación lineal 
\[
  A:K^n \rightarrow K^m
\]
Siendo así, la solución de la ecuación \(A:K^n \rightarrow K^m\). Más aún, la solución de la ecuación homogénea asociada \(Ax=0\) puede verse como el núcleo de la aplicación lineal \(A:K^n \rightarrow K^m\).

El teorema \ref{eq:teorema_dimensión_nucleo_imagen} relativo a aplicaciones lineales nos conduce a la relación:
\[
  \dim(\text{Ker } A = \dim(K^n) - \dim(\text{Im } A) = n - \text{rango } A)
\]
Pero \(n\) es exactamente el número de incógnitas en el sistema homogéneo \(Ax=0\).

\subsection{Aplicaciones lineales singulares y no singulares (isomorfismo)}

Se dice que una aplicación lineal \(F: V \rightarrow U\) es \textit{singular} si la imagen de algún vector no nulo bajo \(F\) es \(0\), es decir, si existe algún \(v \in V\) tal que \(v \neq 0\) pero \(F(v) = 0\). De esta manera, \(F:V \rightarrow U\) es \textit{no singular} si unicamente \(0 \in V\) se aplica en \(0 \in U\) o, equivalentemente, si su núcleo consiste solamente en el vector cero: \(\text{Ker } F = \left\{0\right\}\).

\teorema Supongamos que una aplicación lineal \(F: V \rightarrow U\) es no singular. En tal caso, la imagen de cualquier conjunto linealmente independiente es linealmente independiente.

\textbf{Demostración}: Supongamos que \(v_1, v_2, \cdots, v_n\) son vectores linealmente independientes en \(V\). Afirmamos que \(F(v_1),F(v_2),\cdots, F(v_n)\) son también linealmente independientes. Supongamos que:
\[
  a_1 F(v_1) + a_2 F(v_2) + \cdots + a_n F(v_n) = 0 \quad \text{donde } a_i ~ \text{pertenece a } K
\]
dado que \(F\) es lineal \(F(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n) = 0\) por lo tanto,
\[
  \left(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n\right) \in \text{Ker } F
\]
Pero \(F\) es no singular, esto es el \(\text{Ker }F = \{0\}\), luego \(a_1 v_1 + a_2 v_2 + \cdots + a_n v_n = 0\). Siendo linealmente independientes los \(v_i\), todos los \(a_i\) son cero. De acuerdo con esto los \(F(v_i)\) son linealmente independientes.

\paragraph{Isomorfismos}

Supongamos que una aplicación lineal \(F:V \rightarrow U\) es inyectiva. Entonces solo \(0 \in V\) puede aplicarse en \(0 \in U\) y por tanto \(F\) es no singular. El recíproco es cierto también, porque si \(F\) es no singular y \(F(v)=F(w)\), necesariamente \(F(v-w) = F(v) - F(w) = 0\), luego \(v-w = 0\) o \(v=w\). Así \(F(v) = F(w)\) implica \(v=w\), o sea, \(F\) es inyectiva. Hemos demostrado pues,

\textbf{Proposición}: Una aplicación lineal \(F:V\rightarrow U\) es inyectiva si y solo si es no singular.

Recordemos que una aplicación \(F:V\rightarrow U\) recibe el nombre de isomorfismo si es lineal y biyectiva, esto es, si es lineal, inyectiva y suprayectiva. Recordemos, asimismo, que se dice que un espacio vectorial \(V\) es isomorfo a otro \(U\), escrito \(V \simeq U\), si existe algún isomorfismo \(F:V\rightarrow U\).

En este caso es aplicable el siguiente teorema:
\begin{quote}
  Supongamos que \(V\) tiene dimensión finita y que \(F:V \rightarrow U \) es lineal. Entonces \(F\) es un isomorfismo si y solo si no es singular.

  Demostración:
  Si \(F\) es un isomorfismo, solo \(0\) se aplica en \(0\), de modo que \(F\) es no singular. Supongamos que \(F\) es no singular. Entonces \(\dim(\text{Ker } F) = 0\). Según el teorema (libro:9.5??), \(\dim(V) = \dim(\text{Ker } F) + \dim(\text{Im } F)\).
  Así pues, \(\dim(U) = \dim(V) = \dim(\text{Im } F)\). Como \(U\) tiene dimensión finita, \(\text{Im } F = U\) y por ende \(F\) es suprayectiva. Por lo tanto, \(F\) es simultáneamente inyectiva y suprayectiva, es decir \(F\) es un isomorfismo.
\end{quote}

\subsection{Operaciones con aplicaciones lineales}

Podemos combinar aplicaciones lineales de varias maneras consiguiendo nuevas aplicaciones lineales. Estas operaciones son de gran importancia y se utilizan a lo largo de todo el texto.

Supongamos que \(F:V \rightarrow U\) y \(G:V \rightarrow U\) son aplicaciones lineales entre espacios vectoriales sobre un cuerpo \(K\). Definimos la suma \(F+G\) como la aplicación de \(V\) en \(U\) que asigna \(F(v) + G(v)\) a \(v\in V\):
\[
  (F+G)(v) = F(v) + G (v)
\]
Asimismo, para cualquier escalar \(k \in K\), definimos el producto \(kF\) como la aplicación en \(U\) que asigna \(kF(v)\) a \(v \in V\):
\[
  (kF)(v) = kF (v)
\]
Probamos ahora que si \(F\) y \(G\) son lineales, también lo son \(F + G\) y \(kF\). Tenemos, para todo par de vectores \(v,w \in V\) y todo par de escalares \(a,b \in K\).
\begin{align*}
  (F+G)(av + bw) &= F(av + bw) + G(av + bw) = \\
                 &=aF(v) + bF(w) + aG(v) + bG(w) = \\
                 &=a(F(v) + G(v)) + b(F(w) + G(w)) = \\
                 &=a(F+G)(v) + b(F+G)(w) =\\[3pt]
  (kF)(av + bw) &= kF(av + bw) = k(aF(v)+bF(w))= \\
                &= akF(v) + bkF(w) = a(kF)(v) + b(kF)(w)
\end{align*}
Así pues \(F+G\) y \(kF\) son lineales.

Disponemos del siguiente teorema:
\teorema Sean \(V\) y \(U\) espacios vectoriales sobre un cuerpo \(K\). Ka colección de todas las aplicaciones lineales de \(V\) en \(U\), con las operaciones de suma y producto por un escalar anteriores, es un espacio vectorial sobre \(K\).

El espacio vectorial del teorema suele denotarse por:
\[
\text{Hom}(V,U)
\]
Aquí \(\text{Hom}\) viene de la palabra homomorfismo. En caso de ser \(V\) y \(U\) de dimensión finita, se aplica el teorema que enseguida enunciamos,

Supongamos \(\dim V = m\) y \(\dim U=n\). En tal caso, \(\dim(\text{Hom}(V,U)) = m\cdot n\).

\subsection{Composición de aplicaciones lineales}

Sean \(U, V\) y \(W\) espacios vectoriales sobre un mismo cuerpo \(K\) y \(F : V \rightarrow U\) y \(G: U \rightarrow W\) aplicaciones lineales:
\begin{center}
\begin{tikzpicture}[
  node distance=3cm,
  every node/.style={font=\large},
  set/.style={circle, draw, minimum size=1cm, align=center},
  arrow/.style={-Stealth, thick}
  ]

  \node[set] (V) {\(V\)};
  \node[set, right=of V] (U) {\(U\)};
  \node[set, right=of U] (W) {\(W\)};

  \draw[arrow] (V) -- node[above] {\(F\)} (U);
  \draw[arrow] (U) -- node[above] {\(G\)} (W);

\end{tikzpicture}
\end{center}

  \draw[arrow] (V) -- node[above] {\(f\)} (U);
Recordemos que la función compuesta \(G \circ F\) es la aplicación de \(V\) en \(W\) definida por \((G \circ F) (v) = G(F(v))\). Probemos que \(G\circ F\) es lineal siempre que lo sean \(F\) y \(G\). Tenemos, para todo par de vectores \(v,w \in V\) y todo par de escalares \(a,b \in K\),
\begin{align*}
  (G \circ F)(av + bw) &= G(F(av+bw)) = G(aF(v)+bF(w)) =\\
                       &=aG(F(v)) + bG(F(w)) = a(G \circ F)(v) + b(G \circ F)(w)
  
\end{align*}
Es decir \(G \circ F\) es lineal.

La composición de aplicaciones lineales está relacionada con la suma y el producto por un escalar como sigue:

Sean \(V, U\) y \(W\) espacios vectoriales sobre \(K\), \(F\) y \(F'\) aplicaciones lineales de \(V\) en \(U\), \(G\) y \(G'\) aplicaciones lineales de \(U\) en \(W\) y sea \(k \in K\). Entonces:
\begin{align*}
  G \circ (F+F') = G\circ F + G \circ F'\\
  (G+G') \circ F = G \circ F + G' \circ F \\
  k(G\circ F) = (kG) \circ F = G \circ (kF)
\end{align*}