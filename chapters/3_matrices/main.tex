\section{Matrices}

Si desea repasar los contenidos de matrices de ``Algebra y Geometría Analítica'', de forma resumida, puede ver la sección \ref{sec:repaso_matrices}.

\subsection{Submatrices}

Habitualmente se simboliza a una matriz como hemos estudiado así
\[
A = \begin{bmatrix}
  a_{11} & a_{12} & \cdots & a_{1n}\\
  a_{21} & a_{22} & \cdots & a_{2n}\\
  \vdots & \vdots & \ddots & \vdots \\
  a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
\]

\textbf{Definición}: Se denomina submatriz o bloque de una matriz \(A\), a cualquier matriz obtenida suprimiendo alguna o algunas filas o columnas de \(A\).

\ejemplo{ Si tomamos la matriz \(A\), una submatriz puede ser:}
\[
B = \begin{bmatrix}
  a_{11} & a_{12}\\
  a_{21} & a_{22}
\end{bmatrix}
\]

\subsubsection{Submatriz principal}

Una submatriz de una matriz cuadrada \(A\) se llama submatriz principal si es obtenida de \(A\) eliminando los mismos renglones que columnas. 

\ejemplo{ Si tenemos una matríz A:}
\[
A = \begin{bmatrix}
  2 & 3 & -2 & 1\\
  1 & 3 & 10 & 2\\
  0 & -3 & 1 & 5\\
  -2 & 0 & -2 & 5\\
\end{bmatrix}
\]
Algunas submatrices principales son:
\[
  A_1 =\begin{bmatrix}
    2 & 1 \\
    -2 & 5
  \end{bmatrix},\quad A_2 = \begin{bmatrix}
    2 & 3 & -2 \\
    1 & 3 & 10 \\
    0 & -3 & 1 
  \end{bmatrix}, \quad A_3 = \begin{bmatrix}
    3 & 2\\
    0 & 5 
  \end{bmatrix}
\]
donde en \(A_1\) se han eliminado las columnas \(c_2\) y \(c_3\) y la filas \(f_2\) y \(f_3\). En \(A_2\) la columna \(c_4\) y la fila \(f_4\). Y por último en \(A_3\) las columnas \(c_1\) y \(c_3\) y las filas \(f_1\) y \(f_3\).

\subsubsection{Submatriz principal primera}

Una submatriz principal primera se define solo a partir de las primeras \(r\) filas y columnas de una matriz cuadrada \(A\in\mathbb{R}^{n\times n}\).
Dicho de otro modo:
\[
\text{Submatriz principal primera de orden } r \text{ de } A: \quad A_r = A[1\!:\!r,\,1\!:\!r]
\]
Es decir, se toman los elementos que están en las primeras \(r\) filas y las primeras \(r\) columnas.
\ejemplo{ Para la siguiente matriz \(A\):}
\[
A = \begin{bmatrix}
2 & 3 & -2 & 1\\
1 & 3 & 10 & 2\\
0 & -3 & 1 & 5\\
-2 & 0 & -2 & 5
\end{bmatrix}
\]
Entonces la submatriz principal primera de orden 3 es:
\[
A_3 = \begin{bmatrix}
2 & 3 & -2 \\
1 & 3 & 10\\
0 & -3 & 1
\end{bmatrix} \quad \text{y la de orden 2:} \quad
A_2 = \begin{bmatrix}
2 & 3 \\
1 & 3
\end{bmatrix}
\]
Estas submatrices siempre son cuadradas y mantienen la diagonal principal de la matriz original.

\subsubsection{Submatriz general}

Dada una matriz \(A \in \mathbb{R}^{n \times m}\), una \textit{submatriz general} de orden \(p \times q\) se obtiene seleccionando un subconjunto de \(p\) filas y un subconjunto de \(q\) columnas de \(A\), y conservando los elementos ubicados en las intersecciones correspondientes.

Formalmente, si \(I = \{i_1, i_2, \ldots, i_p\} \subseteq \{1, 2, \ldots, n\}\) y \(J = \{j_1, j_2, \ldots, j_q\} \subseteq \{1, 2, \ldots, m\}\), entonces la submatriz de \(A\) determinada por las filas \(I\) y las columnas \(J\) se denota:
\[
A^{J}_{I} = 
\begin{bmatrix}
a_{i_1 j_1} & a_{i_1 j_2} & \cdots & a_{i_1 j_q} \\
a_{i_2 j_1} & a_{i_2 j_2} & \cdots & a_{i_2 j_q} \\
\vdots & \vdots & \ddots & \vdots \\
a_{i_p j_1} & a_{i_p j_2} & \cdots & a_{i_p j_q}
\end{bmatrix}
\in \mathbb{R}^{p \times q}
\]

\begin{tcolorbox}[interesting_data, title=Detalle]
  Se ha usado una notación estándar para indicar conjuntos de índices: \(I\) para filas, \(J\) para columnas.
  
  Si se tiene una matriz \(M^J_I\), donde \(J\) e \(I\) son conjuntos de índices, representando \(M^{j_1,j_2,\cdots,j_p}_{i_1,i_2,\cdots,i_p}\) que es la submatriz que contiene a los elementos \(a_{i_p j_p}\) cuando \(p\) varía \(1,2,\cdots,p\), y es de orden \(p\times p\).
\end{tcolorbox}

\begin{quote}
  \ejemplo{ Sea la matriz}
  \[
  A = \begin{bmatrix}
  a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
  a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
  a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
  a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
  a_{51} & a_{52} & a_{53} & a_{54} & a_{55}
  \end{bmatrix} \quad \text{la submatriz} \quad
  A^{2,3,5}_{1,2,4} = 
  \begin{bmatrix}
  a_{12} & a_{13} & a_{15} \\
  a_{22} & a_{23} & a_{25} \\
  a_{42} & a_{43} & a_{45}
  \end{bmatrix}
  \]
  se obtiene seleccionando las filas 1, 2 y 4, y las columnas 2, 3 y 5.
\end{quote}

\subsubsection{Menores de submatrices}

Sea \(A \in \mathbb{R}^{n \times n}\) una matriz cuadrada. Se define como:

\begin{itemize}
  \item \textbf{Menor}: es el \textit{determinante} de una submatriz cuadrada de \(A\), obtenida al seleccionar un subconjunto de filas \(I = \{i_1, \ldots, i_k\}\) y un subconjunto de columnas \(J = \{j_1, \ldots, j_k\}\). Se denota:
  \[
  M = \det\left(A^{J}_{I}\right) = 
  \begin{vmatrix}
  a_{i_1 j_1} & \cdots & a_{i_1 j_k} \\
  \vdots & \ddots & \vdots \\
  a_{i_k j_1} & \cdots & a_{i_k j_k}
  \end{vmatrix}
  \]

  \item \textbf{Menor con signo}: es el menor anterior multiplicado por una potencia de \((-1)\) determinada por la suma de los índices de fila y columna seleccionados:
  \[
  \text{Menor con signo} = (-1)^{\sum I + \sum J} \cdot \det\left(A^{J}_{I}\right)
  \]
  donde \(\sum I = i_1 + \cdots + i_k\), \(\sum J = j_1 + \cdots + j_k\).

  \item \textbf{Menor complementario}: es el determinante de la submatriz cuadrada obtenida al \textbf{eliminar} del conjunto total de filas y columnas los índices de \(I\) y \(J\), es decir, se considera la submatriz \(A^{J^c}_{I^c}\), donde \(I^c\) y \(J^c\) son los complementarios de \(I\) y \(J\) respecto del conjunto \(\{1,\ldots,n\}\).
\end{itemize}

\begin{quote}
  \ejemplo{ Dada la siguiente matriz, plantear el menor, el menor con signo y el menor complementario}
  \[
  A = \begin{bmatrix}
  a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
  a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
  a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
  a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
  a_{51} & a_{52} & a_{53} & a_{54} & a_{55}
  \end{bmatrix}
  \]
  Si seleccionamos las filas \(I = \{1,2,4\}\) y las columnas \(J = \{2,3,5\}\), entonces:
  \[
  A^{2,3,5}_{1,2,4} = 
  \begin{bmatrix}
  a_{12} & a_{13} & a_{15} \\
  a_{22} & a_{23} & a_{25} \\
  a_{42} & a_{43} & a_{45}
  \end{bmatrix}
  \Rightarrow 
  \det(A^{2,3,5}_{1,2,4}) = \text{menor}
  \]

  \[
  \text{Menor con signo} = (-1)^{1+2+4 + 2+3+5} \cdot \det(A^{2,3,5}_{1,2,4}) = (-1)^{17} \cdot \det(A^{2,3,5}_{1,2,4})
  \]

  El \textbf{menor complementario} se obtiene considerando las filas \(I^c = \{3,5\}\) y columnas \(J^c = \{1,4\}\):
  \[
  A^{1,4}_{3,5} = \begin{bmatrix}
  a_{31} & a_{34} \\
  a_{51} & a_{54}
  \end{bmatrix}
  \Rightarrow
  \det(A^{1,4}_{3,5}) = \text{menor complementario}
  \]
\end{quote}

\subsection{Matrices por bloques}

En muchas aplicaciones prácticas, especialmente cuando se trabaja con matrices grandes, resulta útil pensar una matriz como compuesta por \textit{submatrices} o \textit{bloques}. Esta descomposición permite representar y operar sobre matrices de forma más compacta y eficiente.

\subsubsection{Representaciones por filas o columnas}

Una matriz puede pensarse en términos de sus columnas o filas como bloques individuales.
\begin{quote}
  \ejemplo{ Sea la matriz:}
  \[
  A = \begin{bmatrix}
  2 & 0 & -1\\
  1 & 3 & 2
  \end{bmatrix}
  \]
  Podemos representarla en términos de sus columnas:
  \[
  C_1 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \quad
  C_2 = \begin{bmatrix} 0 \\ 3 \end{bmatrix}, \quad
  C_3 = \begin{bmatrix} -1 \\ 2 \end{bmatrix}
  \Rightarrow A = [C_1 \; C_2 \; C_3]
  \]
  O bien, en términos de sus filas:
  \[
  F_1 = \left(2, 0, -1\right), \quad
  F_2 = \left(1, 3, 2\right)
  \Rightarrow A = \begin{bmatrix} F_1 \\ F_2 \end{bmatrix}
  \]
\end{quote}

\subsubsection{Partición por bloques}

Más generalmente, podemos dividir una matriz en bloques rectangulares, llamados \textbf{submatrices}. Se dice que una matriz está \textit{particionada por bloques} cuando se trazan líneas horizontales y verticales (imaginarias o visibles) que dividen la matriz completamente.

\textbf{Definición:} Sea \(A \in \mathbb{R}^{m \times n}\). Decimos que \(A\) está \textbf{particionada propiamente en bloques} si puede escribirse como una matriz de submatrices de la forma:
\[
A = \begin{bmatrix}
A_{11} & A_{12} & \cdots & A_{1p} \\
A_{21} & A_{22} & \cdots & A_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
A_{q1} & A_{q2} & \cdots & A_{qp}
\end{bmatrix}
\]
donde cada \(A_{ij}\) es una submatriz.  

El número de filas del bloque \(A_{ij}\) depende solo del índice \(i\), y el número de columnas depende solo de \(j\); es decir, los bloques de una misma fila tienen el mismo número de filas, y los de una misma columna el mismo número de columnas, lo que asegura la consistencia del tamaño global.

En otras palabras, por ejemplo, si estás en la primera fila de bloques \((i=1)\), entonces todos los bloques \(A_{1j}\), sin importar el \(j\), deben tener igual cantidad de filas. De la misma forma para las columnas.

\begin{quote}
  \ejemplo{ Consideremos la matriz:}
  \[
  A = \left[\begin{array}{c|cc|cc}
  1 & 2 & 3 & 4 & 5 \\
  2 & 3 & 4 & 5 & 6 \\
  3 & 4 & 5 & 6 & 7 \\
  \hline
  4 & 5 & 6 & 7 & 8
  \end{array}\right]
  \]
  La matriz está particionada horizontal y verticalmente, formando submatrices rectangulares. Podemos escribirla como una matriz por bloques:
  \[
  A = \begin{bmatrix}
  A_{11} & A_{12} & A_{13} \\
  A_{21} & A_{22} & A_{23}
  \end{bmatrix}
  \]
  donde los bloques son:
  \[
  \begin{aligned}
  A_{11} &= \begin{bmatrix} 1 \end{bmatrix}, \quad &
  A_{12} &= \begin{bmatrix} 2 & 3 \end{bmatrix}, \quad &
  A_{13} &= \begin{bmatrix} 4 & 5 \end{bmatrix} \\
  A_{21} &= \begin{bmatrix} 4 \end{bmatrix}, \quad &
  A_{22} &= \begin{bmatrix} 5 & 6 \end{bmatrix}, \quad &
  A_{23} &= \begin{bmatrix} 7 & 8 \end{bmatrix}
  \end{aligned}
  \]
  En este caso, \(A\) está particionada en \(2 \times 3\) bloques.
\end{quote}

\subsubsection{Suma de matrices por bloques}

\textbf{Definición}: Se define la suma de dos matrices descompuestas en bloques como la matriz por bloques que tiene en la posición \((i, j)\) la suma de los bloques que ocupan esa posición, es decir:
\[
 (A+B)_{ij} = A_{ij} + B_{ij}
\]
Para que la suma por bloques pueda realizarse, las dos matrices deben ser del mismo tamaño y han de estar descompuestas en el mismo número de bloques fila y columna, y los bloques que ocupan la misma posición han de ser a su vez del mismo tamaño.

\ejemplo{ Sean las matrices \(A\) y \(B\) como sigue:}
\[
  A= \left[\begin{array}{c|cc|cc}
    1 & 2 & 3 & 4 & 5 \\
    2 & 3 & 4 & 5 & 6 \\
    3 & 4 & 5 & 6 & 7 \\
    \hline
    4 & 5 & 6 & 7 & 8
  \end{array}\right] \qquad 
  B= \left[\begin{array}{c|cc|cc}
    0 & 1 & 3 & 1 & 2 \\
    1 & 2 & 4 & 3 & 0 \\
    2 & 4 & 2 & 0 & 2 \\
    \hline
    0 & 1 & 0 & 2 & 1
  \end{array}\right]
\]
la suma \(A+B\) será igual a: 
\[
A + B = \left[\begin{array}{c|cc|cc}
  1 & 3 & 6 & 5 & 7 \\
  3 & 5 & 8 & 8 & 6 \\
  5 & 8 & 7 & 6 & 9 \\
  \hline
  4 & 6 & 6 & 9 & 9
\end{array}\right]
\]

\subsubsection{Producto de matrices por bloques}

El producto de matrices por bloques se realiza, formalmente, igual que si fuera por elementos.

\textbf{Definición}: Se define el producto de dos matrices \(A\) y \(B\) descompuestas en bloques como la matriz por bloques \(C\) que tiene en la posición \((i,j)\) el bloque:
\[
  C_{ij} = \sum_{k=1}^r A_{ik} \cdot A_{kj}
\]

Para que el producto por bloques pueda realizarse, las dos matrices \(A\) y \(B\) deben estar descompuestas en bloques de forma conforme, es decir, el número de bloques columna de la matriz \(A\) debe ser igual que el de bloques fila de la matriz \(B\), y los bloques involucrados en la suma anterior han de ser de tamaño adecuado para que se puedan multiplicar por elementos.

\ejemplo{ Sean las matrices \(A_{2\times 3}\) y \(B_{3 \times 2}\) como sigue:}
\[
A = \left[\begin{array}{c|cc|cc}
1 & 2 & 3 & 4 & 5 \\
2 & 3 & 4 & 5 & 6 \\
3 & 4 & 5 & 6 & 7 \\
\hline
4 & 5 & 6 & 7 & 8  
\end{array}\right] = \begin{bmatrix}
  A_{11} & A_{12} & A_{13}\\
  A_{21} & A_{22} & A_{23}
\end{bmatrix}\qquad B = \left[\begin{array}{c|cc}
1 & 2 & 3 \\
\hline
2 & 3 & 4 \\
3 & 4 & 5 \\
\hline
4 & 5 & 6 \\
5 & 6 & 7
\end{array}\right] = \begin{bmatrix}
  B_{11} & B_{12} \\
  B_{21} & B_{22} \\
  B_{31} & B_{32}
\end{bmatrix}
\]
la matriz \(A \cdot B = C\) será de orden \(2 \times 2\) bloques y se calcula de la siguiente forma:
\[
C = \begin{bmatrix}
  \left(A_{11} \cdot B_{11} + A_{12} \cdot B_{21} + A_{13} \cdot B_{31}\right) & \left(A_{11} \cdot B_{12} + A_{12} \cdot B_{22} + A_{13} \cdot B_{32}\right)\\
  \left(A_{21} \cdot B_{11} + A_{22} \cdot B_{21} + A_{23} \cdot B_{31}\right) & \left(A_{21} \cdot B_{12} + A_{222} \cdot B_{22} + A_{23} \cdot B_{32}\right)
\end{bmatrix}
\]
Y ahora calculamos el resultado de cada elemento \(C_{ij}\) de la matriz:
\begin{itemize}
  \item \(C_{11} = \begin{bmatrix}
    1 \\ 2 \\ 3
  \end{bmatrix} \cdot \left[1\right] + \begin{bmatrix}
    2 & 3 \\
    3 & 4 \\
    4 & 5
  \end{bmatrix} \cdot \begin{bmatrix}
    2 \\ 3
  \end{bmatrix} + \begin{bmatrix}
    4 & 5 \\
    5 & 6 \\
    6 & 7
  \end{bmatrix} + \begin{bmatrix}
    4 & 5
  \end{bmatrix} = \begin{bmatrix}
    55 \\ 75 \\ 85
  \end{bmatrix}\)
  \item \(C_{12} = \begin{bmatrix}
    1 \\ 2 \\ 3 
  \end{bmatrix}\cdot \begin{bmatrix}
    2 & 3
  \end{bmatrix} + \begin{bmatrix}
    2 & 3 \\
    3 & 4 \\
    4 & 5
  \end{bmatrix} \cdot \begin{bmatrix}
    3 & 4 \\
    4 & 5
  \end{bmatrix} + \begin{bmatrix}
    4 & 5 \\
    5 & 6 \\
    6 & 7
  \end{bmatrix} \cdot \begin{bmatrix}
    5 & 6 \\
    5 & 7
  \end{bmatrix} = \begin{bmatrix}
    70 & 85 \\
    80 & 115 \\
    121 & 135
  \end{bmatrix}\)
  \item \(C_{21} = \left[4\right] \cdot \left[1\right] + \begin{bmatrix}
    5 & 6
  \end{bmatrix} \cdot \begin{bmatrix}
    2 \\ 3
  \end{bmatrix} + \begin{bmatrix}
    7 & 8 
  \end{bmatrix} \cdot \begin{bmatrix}
    4 & 5
  \end{bmatrix} = [100]\)
  \item \(C_{22} = [4] \cdot \begin{bmatrix}
    2 & 3
  \end{bmatrix} + \begin{bmatrix}
    5 & 6
  \end{bmatrix} \cdot \begin{bmatrix}
    3 & 4 \\ 
    4 & 5
  \end{bmatrix} + \begin{bmatrix}
    7 & 8
  \end{bmatrix} \cdot \begin{bmatrix}
    5 & 6 \\ 
    6 & 7
  \end{bmatrix}= \begin{bmatrix}
    138 & 160
  \end{bmatrix}\)
\end{itemize}
Resultando entonces:
\[
A \cdot B = C = \left[\begin{array}{c|cc}
  55 & 70 & 85\\
  75 & 60 & 115 \\
  85 & 121 & 135 \\
  \hline
  100 & 138 & 160
\end{array}\right]
\]

La ventaja de esta forma de proceder es que para obtener cada bloque del producto solo se manipulan matrices más pequeñas.

La idea de las matrices por bloques conduce a entender una matriz \(A\) de orden \(p \times q\) como un conjunto de \(q\) columnas cada una de las cuales es un vector de \(p\) componentes; o bien, un conjunto de \(p\) filas, cada una de las cuales es un vector de \(q\) componentes, \[
A \cdot B = \begin{bmatrix}
  a_{1} \\ a_{2} \\ \vdots \\ a_p 
\end{bmatrix} \cdot \begin{bmatrix}
  b_1 & b_2 & \cdots & b_q
\end{bmatrix}
\]
de manera que \((A \cdot B)_{ij} = a_i \cdot b_j\), es decir, en el lenguaje del algebra elemental, cada elemento del producto es el producto escalar de la fila i-ésima por la columna j-ésima.

\subsubsection{Transpuesta de matrices por bloques}

\textbf{Definición}: Sea la matriz por bloques \(A\):
\[
  A= \begin{bmatrix}
    A_{11} & A_{12} & A_{13} \\
    A_{21} & A_{22} & A_{23}
  \end{bmatrix}
\]
su matriz transpuesta \(A^T\) por bloques se determina transponiendo cada bloque individual y luego se transpone la matriz de bloques \(A\) de la siguiente forma:
\[
A = \begin{bmatrix}
  A_{11}^T & A_{21}^T \\
  A_{12}^T & A_{22}^T \\
  A_{13}^T & A_{23}^T
\end{bmatrix}
\]

\ejemplo{ Dada la matriz \(A\), obtener la matriz transpuesta}
\[
  A = \left[\begin{array}{ccc|cc}
    2 & -3 & 1 & 0 & -4 \\
    1 & 5 & -2 & 3 & -1 \\
    \hline
    0 & -4 & -2 & 7 & -1
  \end{array}\right] \quad \rightarrow \quad A^T = \left[\begin{array}{cc|c}
    2 & 1 & 0 \\
    -3 & 5 & -4 \\
    1 & -2 & -2 \\
    \hline
    0 & 3 & 7 \\
    -4 & -1 & -1
  \end{array}\right] 
\]

\subsubsection{Determinante de matrices cuadradas por bloques}

\paragraph{Matriz cuadrada por bloques}

Una matriz \(A\) se llama cuadrada por bloques si:
\begin{itemize}
  \item \(A\) es una matriz cuadrada
  \item Si los bloques diagonales son matrices cuadradas
\end{itemize}

\ejemplo{ Obsérvense las siguientes matrices:}

\[
  A = \left[\begin{array}{cc|cc|c}
    1 & 2 & 3 & 4 & 5 \\
    1 & 1 & 1 & 1 & 1 \\
    \hline
    9 & 8 & 7 & 6 & 5 \\
    \hline
    4 & 4 & 4 & 4 & 4 \\
    3 & 5 & 3 & 5 & 3
  \end{array}\right] \qquad B = \left[\begin{array}{cc|cc|c}
        1 & 2 & 3 & 4 & 5 \\
    1 & 1 & 1 & 1 & 1 \\
    \hline
    9 & 8 & 7 & 6 & 5 \\
    4 & 4 & 4 & 4 & 4 \\
    \hline
    3 & 5 & 3 & 5 & 3
  \end{array}\right]
\]
\(A\) no es cuadrada por bloques, ya que los bloques diagonales \(A_{22}\) y \(A_{33}\) no son matrices cuadradas. \(B\) si es una matriz cuadrada por bloques.

\paragraph{Matriz diagonal por bloques}

Una matriz cuadrada \(A\) dada en bloques en la que todos los bloques que no están en la diagonal principal son iguales a cero se denomina matriz diagonal por bloques. Es decir, \(A_{ij} = 0\) para todo \(i \neq j\):\[
A = \begin{bmatrix}
  A_{11} & 0 & \cdots & 0 \\
  0 & A_{22} & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \cdots & A_{mn}
\end{bmatrix}
\]

\paragraph{Matriz triangular inferior por bloques}

Una matriz \(A\) cuadrada dada en bloques en la que los bloques por encima de su diagonal principal son cero se llama matriz triangular inferior por bloques. Es decir, \(A_{ij} = 0\) para todo \(j > i\).
\[
A = \begin{bmatrix}
  A_{11} & 0 & \cdots & 0 \\
  A_{21} & A_{22} & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  A_{n1} & A_{n2} & \cdots & A_{mn}
\end{bmatrix}
\]

\paragraph{Matriz triangular superior por bloques}

Una matriz cuadrada \(B\) descompuesta en bloques en la que los bloques por debajo de su diagonal principal son cero se llama matriz triangular superior por bloques. Es decir, \(B_{ij} = 0\) para todo \(j < i\).
\[
B = \begin{bmatrix}
  B_{11} & B_{12} & \cdots & B_{1m} \\
  0 & B_{22} & \cdots & B_{2m} \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \cdots & B_{mn}
\end{bmatrix}
\]

\paragraph{Cálculo del determinante}

Para las matrices triangulares (superior e inferior) por bloques \(A\), así como para todas las matrices diagonales por bloques, se cumple que:

\[
  \det(A) = \prod_{k=1}^{r}\det(A_{kk})
\]
\teorema{Si \(A\) es una matriz triangular superior (o inferior) por bloques, con bloques diagonales \(A_{11}, A_{22}, \cdots, A_{pp}\), entonces el determinante de \(A\) es el producto de los determinantes de los bloques diagonales}
\label{teo:determinante_de_matrices_por_bloques}
\[
\det(A) = \det(A_{11}) \cdot \det(A_{22}) \cdot ~~ \cdots ~~ \cdot \det(A_{pp})
\]
\begin{quote}  
  \ejemplo{ Veamos un ejemplo de determinantes por bloques:}
  \[
  A = \left[\begin{array}{cc|ccc}
    2 & 3 & 4 & 7 & 8 \\
    -1 & 5 & 3 & 2 & 1 \\
    \hline
    0 & 0 & 2 & 1 & 5 \\
    0 & 0 & 3 & -1 & 4 \\
    0 & 0 & 5 & 2 & 6 
  \end{array}\right]
  \]
  \(A\) es una matriz triangular superior por bloques. Los bloques diagonales son dos, calculamos el determinante de \(A\) usando el teorema \ref{teo:determinante_de_matrices_por_bloques}:
  \[
  \det(A) = \det(A_{11}) \cdot \det(A_{22}) = \begin{vmatrix}
    2 & 3 \\ 
    -1 & 5
  \end{vmatrix} \cdot \begin{vmatrix}
    2 & 1 & 5 \\
    3 & -1 & 4 \\
    5 & 2 & 6
  \end{vmatrix} = 13 \cdot 29 = 377
  \]
\end{quote}

\subsection{Matrices complejas}

\textbf{Definición}: Una matriz se denomina compleja si alguno de sus elementos es un número complejo.

\ejemplo{ La matriz \(A\) es una matriz compleja de orden \(2\):}
\[
A = \begin{bmatrix}
  2+3i & 5i \\
  4-3i & 7+8i
\end{bmatrix}
\]
\subsubsection{Matrices hermíticas}

\textbf{Definición:}  
Una matriz compleja cuadrada \(A \in \mathbb{C}^{n \times n}\) se denomina \textit{hermítica} (o \textit{hermitiana}) si es igual a su transpuesta conjugada, es decir:
\[
A = (\bar{A})^T
\]
donde \(\bar{A}\) denota la matriz conjugada de \(A\) y \((\cdot)^T\) la transposición.

\begin{quote}
  \ejemplo{ forma general de una matriz hermítica de orden 3}

  Consideramos una matriz compleja genérica de orden 3:
  \[
  A = \begin{bmatrix}
  a_{11} + ib_{11} & a_{12} + ib_{12} & a_{13} + ib_{13} \\
  a_{21} + ib_{21} & a_{22} + ib_{22} & a_{23} + ib_{23} \\
  a_{31} + ib_{31} & a_{32} + ib_{32} & a_{33} + ib_{33}
  \end{bmatrix}
  \]

  Para que sea hermítica, debe cumplirse \(A = (\bar{A})^T\). Al igualar ambos lados, obtenemos:

  - En la diagonal principal:
  \[
  a_{jj} + ib_{jj} = a_{jj} - ib_{jj} \quad \Rightarrow \quad b_{jj} = 0 \quad \forall j
  \]
  Es decir, los elementos de la diagonal deben ser reales.

  - Fuera de la diagonal, por ejemplo en la posición \((1,2)\) y \((2,1)\):
  \[
  a_{12} + ib_{12} = a_{21} - ib_{21} \quad \Rightarrow \quad a_{12} = a_{21}, \quad b_{12} = -b_{21}
  \]
  De forma general, se deduce:
  \[
  a_{ij} = a_{ji}, \quad b_{ij} = -b_{ji} \quad \text{para } i \ne j
  \]

  Entonces la forma general de una matriz hermítica de orden 3

  \[
  A = \begin{bmatrix}
  a_{11} & a_{12} + ib_{12} & a_{13} + ib_{13} \\
  a_{12} - ib_{12} & a_{22} & a_{23} + ib_{23} \\
  a_{13} - ib_{13} & a_{23} - ib_{23} & a_{33}
  \end{bmatrix}
  \quad \text{con } a_{ij}, b_{ij} \in \mathbb{R}
  \]
  Esto puede generalizarse para matrices de orden \(n\)
\end{quote}

\paragraph{Observación: relación con matrices reales}

Si \(A\) es una matriz de coeficientes reales, entonces:
\[
\bar{A} = A \quad \Rightarrow \quad A \text{ es hermítica } \Leftrightarrow A^T = A
\]
Es decir, en el caso real, las matrices hermíticas coinciden con las matrices simétricas.

\subsubsection{Matrices antihermitianas}

\textbf{Definición:}  
Una matriz compleja cuadrada \(A \in \mathbb{C}^{n \times n}\) se denomina \textit{antihermítica} si es igual a la opuesta de su transpuesta conjugada:
\[
A = -(\bar{A})^T
\]

Este tipo de matrices se caracteriza porque su diagonal principal contiene solo elementos \textit{imaginarios puros} y sus entradas fuera de la diagonal cumplen:
\[
a_{ij} = -\bar{a}_{ji}
\]

\paragraph{Ejemplo: forma general de una matriz antihermítica de orden 2}

\[
A = \begin{bmatrix}
ai & b + ci \\
- b + ci & di
\end{bmatrix}
\quad \text{con } a, b, c, d \in \mathbb{R}
\]

---

\subsubsection{Propiedades de las matrices hermíticas}

\begin{itemize}
  \item Toda matriz real simétrica es hermítica.
  \item Toda matriz hermítica tiene entradas reales en la diagonal.
  \item La suma (o resta) de dos matrices hermíticas es otra matriz hermítica.
  \item Si \(\lambda \in \mathbb{R}\), entonces \(\lambda A\) es hermítica.
  \item El producto de dos matrices hermíticas no necesariamente es hermítico.
  \item Si una matriz hermítica es invertible, su inversa también es hermítica.
  \item El determinante de una matriz hermítica es un número real.
  \item Toda matriz compleja \(A\) puede escribirse como:
  \[
  A = H + K
  \]
  donde
  \[
  H = \frac{A + (\bar{A})^T}{2} \text{ es hermítica, y }
  \]
  \[
  K = \frac{A - (\bar{A})^T}{2} \text{ es antihermítica}
  \]
  En particular:
  \[
  A + (\bar{A})^T \text{ es hermítica}, \qquad A - (\bar{A})^T \text{ es antihermítica}
  \]
\end{itemize}

\subsubsection{Matrices unitarias}

\textbf{Definición}: una matriz unitaria es una matriz cuadrada compleja \(A \in \mathbb{C}^{n \times n}\) que verifica:
\[
A^\dagger A = A A^\dagger = I
\]
donde \(A^\dagger = (\bar{A})^T\) es la conjugada transpuesta de \(A\).

En palabras, la definición se lee: ``una matriz es unitaria cuando el producto de ella por su conjugada transpuesta es la matriz identidad''.

Esto significa que, toda matriz unitaria es \textbf{no} singular, es decir, tiene inversa, y la inversa \textbf{es su transpuesta conjugada}. 

\begin{tcolorbox}[remember, title=Recordatorio]
  Una matriz singular es aquella que no posee inversa. Contrariamente una matriz \textbf{no} singular es aquella que si tiene inversa.
\end{tcolorbox}

\begin{quote}
  \ejemplo{ La siguiente matriz es unitaria:}
  \[
    A = \frac{1}{3} \begin{bmatrix}
      2 & -2+i \\
      2+i & 2
    \end{bmatrix}
  \]
  ya que:
  \[
  A \cdot (\bar{A})^T = \frac{1}{3} \begin{bmatrix}
      2 & -2+i \\
      2+i & 2
    \end{bmatrix} \cdot \frac{1}{3} \begin{bmatrix}
      2 & -2-i \\
      -2-i & 2
    \end{bmatrix} = I
  \]
\end{quote}

\newpage

\paragraph{Propiedades}

\begin{itemize}
  \item El valor absoluto del determinante de una matriz unitaria es siempre igual a 1.
  \item La matriz identidad es una matriz unitaria.
  \item El producto de dos matrices unitarias del mismo orden da como resultado otra matriz unitaria.
\end{itemize}

\subsubsection{Matrices normales}

Una matriz normal es una matriz cuadrada compleja que conmuta con su conjugada transpuesta, es decir:
\[
  A^\dagger A = A A^\dagger ~~ \text{ o equivalentemente } ~~ (\bar{A})^T A = A (\bar{A})^T
\]

\ejemplo{ La matriz \(A\) es normal, ya que cumple con la definición}
\[
  A = \begin{bmatrix}
    i & i \\ i & -i
  \end{bmatrix}
\]

\paragraph{Propiedades}

\begin{itemize}
  \item Toda matriz unitaria es también una matriz normal.
  \item Del mismo modo, una matriz hermitiana es una matriz normal.
  \item Igualmente, una matriz antihermitiana es una matriz normal.
  \item Si una matriz está compuesta únicamente por números reales y es simétrica, también es una matriz normal.
  \item De igual modo, una matriz real antisimétrica también es una matriz normal.
  \item Toda matriz hermítica es una matriz normal. Aunque no todas las matrices normales son matrices hermíticas.
\end{itemize}

\subsection{Factorización Lower Upper de matrices}

Tanto el método de Doolittle como el método de Crout son variantes del mismo procedimiento: la descomposición LU de una matriz. Ambos métodos buscan descomponer una matriz cuadrada \(A\) en el producto de dos matrices:
\[
A = LU
\]
donde:
\begin{itemize}
  \item \(L\) es una matriz triangular inferior (con ceros arriba de la diagonal),
  \item \(U\) es una matriz triangular superior (con ceros debajo de la diagonal),
\end{itemize}
pero se diferencian en cómo se normaliza la descomposición, es decir, en qué matriz se colocan los unos de la diagonal.

\subsubsection{Método de Doolittle}

En este método la matriz \(L\) será triangular inferior con unos en la diagonal.

La matriz \(U\) es triangular superior sin restricciones especiales.
\[
L = \begin{bmatrix}
1 & 0 & 0 & \dots \\
\ell_{21} & 1 & 0 & \dots \\
\ell_{31} & \ell_{32} & 1 & \dots \\
\vdots & \vdots & \vdots & \ddots \\
\end{bmatrix},
\quad
U = \begin{bmatrix}
u_{11} & u_{12} & u_{13} & \dots \\
0 & u_{22} & u_{23} & \dots \\
0 & 0 & u_{33} & \dots \\
\vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}
\]

Es útil para resolver sistemas de ecuaciones lineales \(A\vec{x} = \vec{b}\).
Primero se resuelve:
\[
LU\vec{x} = \vec{b}
\Rightarrow
\begin{cases}
L\vec{y} = \vec{b} \quad \text{(sustitución hacia adelante)} \\
U\vec{x} = \vec{y} \quad \text{(sustitución hacia atrás)}
\end{cases}
\]

\subsubsection{Método de Crout}

La matriz \(U\) será triangular superior con unos en la diagonal.
La matriz \(L\) es triangular inferior sin restricciones.
\[
L = \begin{bmatrix}
\ell_{11} & 0 & 0 & \dots \\
\ell_{21} & \ell_{22} & 0 & \dots \\
\ell_{31} & \ell_{32} & \ell_{33} & \dots \\
\vdots & \vdots & \vdots & \ddots \\
\end{bmatrix},
\quad
U = \begin{bmatrix}
1 & u_{12} & u_{13} & \dots \\
0 & 1 & u_{23} & \dots \\
0 & 0 & 1 & \dots \\
\vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}
\]

Ambos métodos requieren que la matriz \(A\) sea cuadrada y no singular (invertible).

\subsubsection{Descomposición de matrices: LU (método de Doolittle)}

El método de Doolittle consiste en encontrar los valores de los elementos de la matriz \(L\) y \(U\) a partir de la matriz \(A\), para después resolver el sistema \(A\vec{x} = \vec{b}\)

Se efectúan las siguientes sustituciones:
\begin{align*}
  LUx &= b \\
  Lz &= b \quad \text{donde} \quad z=Ux
\end{align*}
De tal manera que:
\begin{enumerate}
  \item Se calcula \(z\) a partir de \(Lz=b\)
  \item Se calcula \(x\) a partir de \(Ux=z\)
\end{enumerate}

\begin{quote}
  \ejemplo{ Veamos el siguiente ejercicio (es un ejercicio demostrativo, para entender la ventaja de este método, más adelante explicaremos la implementación)}
  \label{ej:factorizacion_lu}

  Resolver el siguiente sistema de ecuaciones:
  \begin{align*}
    \begin{cases}
      1x_1 + 2x_2 + 4x_3 + 1x_4 = 21 \\
      2x_1 + 8x_2 + 6x_3 + 4x_4 = 52 \\
      3x_1 + 10x_2 + 8x_3 + 8x_4 = 79 \\
      4x_1 + 12x_2 + 10x_3 + 6x_4 = 82
    \end{cases}
  \end{align*}
  Las matrices asociadas a el SEL son (suponiendo que conocemos las matrices \(L\) y \(U\)):
  \[
  A = \begin{bmatrix}
    1 & 2 & 4 & 1 \\
    2 & 8 & 6 & 4 \\
    3 & 10 & 8 & 8 \\
    4 & 12 & 10 & 6
  \end{bmatrix} = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    2 & 1 & 0 & 0 \\
    3 & 1 & 1 & 0 \\ 
    4 & 1 & 2 & 1
  \end{bmatrix} \cdot \begin{bmatrix}
    1 & 2 & 4 & 1 \\
    0 & 4 & -2 & 2 \\
    0 & 0 & -2 & 3 \\
    0 & 0 & 0 & -6
  \end{bmatrix}
  \]
  Primero resolvemos el sistema de ecuaciones correspondiente al sistema triangular inferior \(Lz=b\):
  \begin{align*}
    \begin{cases}
      \phantom{1}z_1 \phantom{ + y_2 + y_3 + y_4 00} = 21 \\
      2z_1 +z_2 \phantom{+ 0y_3 + y_4 0} = 52 \\
      3z_1 +z_2 + z_3 \phantom{+ y_4 00} = 79 \\
      4z_1 +z_2 + 2z_3 + z_4 = 82
    \end{cases}
  \end{align*}
  Resultando:
  \[z_1 = 21 \qquad z_2 = 10 \qquad z_3 = 6 \qquad z_4 = -24\]
  y posteriormente solucionamos \(Ux=z\):
    \begin{align*}
    \begin{cases}
      x_1 +2x_2 + 4x_3 + \phantom{0}x_4 = 21 \\
      \phantom{0x_1 +} 4 x_2 - 2x_3 + 2x_4 = 10 \\
      \phantom{x_1 + 0x_2} - 2x_3 + 3x_4 = 6 \\
      \phantom{x_1 + x_2 + 0x_30}-6x_4 = -24
    \end{cases}
  \end{align*}
  Resolviendo igual que antes, usando sustitución hacia atrás tenemos:
  \[x_1 = 1 \qquad x_2 = 2 \qquad x_3 = 3 \qquad x_4 = 4\]
\end{quote}

\begin{tcolorbox}[title=Usos de este método]
  Tanto el método de Doolittle como el método de Crout son muy usados para resolver SELs con computadoras, ya que permiten fácilmente la automatización y son bastante eficientes. Vea como de rápido se ha resuelto el SEL del ejemplo \ref{ej:factorizacion_lu}
\end{tcolorbox}

Ahora que hemos visto el potencial de este tipo de métodos, veamos cómo factorizamos una matriz con un ejemplo.

\paragraph{Implementación}

Repasando, el método de Doolittle consiste en descomponer una matriz cuadrada \(A\) como el producto de dos matrices:
\[
A = LU
\]
donde:
\begin{itemize}
  \item \(L\) es una matriz triangular inferior con unos en su diagonal.
  \item \(U\) es una matriz triangular superior.
\end{itemize}

Una vez obtenidas \(L\) y \(U\), se resuelve el sistema \(A\vec{x} = \vec{b}\) de forma eficiente mediante dos pasos:

\begin{align*}
LUx &= b \\
Lz &= b \quad &\text{(sustitución hacia adelante)} \\
Ux &= z \quad &\text{(sustitución hacia atrás)}
\end{align*}

\begin{quote}
  \ejemplo{ Resolver el siguiente sistema de ecuaciones aplicando el método de Doolittle:}

  \begin{align*}
  \begin{cases}
  2x_1 + 3x_2 = 8 \\
  4x_1 + 7x_2 = 18
  \end{cases}
  \end{align*}
  La matriz asociada es:
  \[
  A = \begin{bmatrix}
  2 & 3 \\
  4 & 7
  \end{bmatrix}
  \]
  Buscamos \(L\) y \(U\) tales que \(A = LU\), con:
  \[
  L = \begin{bmatrix}
  1 & 0 \\
  \ell_{21} & 1
  \end{bmatrix}, \quad
  U = \begin{bmatrix}
  u_{11} & u_{12} \\
  0 & u_{22}
  \end{bmatrix}
  \]
  Multiplicamos:
  \[
  LU = \begin{bmatrix}
  1 & 0 \\
  \ell_{21} & 1
  \end{bmatrix}
  \begin{bmatrix}
  u_{11} & u_{12} \\
  0 & u_{22}
  \end{bmatrix}
  = \begin{bmatrix}
  u_{11} & u_{12} \\
  \ell_{21} u_{11} & \ell_{21} u_{12} + u_{22}
  \end{bmatrix}
  \]
  Igualamos con \(A\):
  \[
  \begin{bmatrix}
  u_{11} & u_{12} \\
  \ell_{21} u_{11} & \ell_{21} u_{12} + u_{22}
  \end{bmatrix}
  =
  \begin{bmatrix}
  2 & 3 \\
  4 & 7
  \end{bmatrix}
  \]
  Comparando entradas:
  \begin{align*}
  u_{11} &= 2 \\
  u_{12} &= 3 \\
  \ell_{21} u_{11} &= 4 \quad \Rightarrow \quad \ell_{21} = 2 \\
  \ell_{21} u_{12} + u_{22} &= 7 \quad \Rightarrow \quad 2\cdot 3 + u_{22} = 7 \Rightarrow u_{22} = 1
  \end{align*}
  Entonces:
  \[
  L = \begin{bmatrix}
  1 & 0 \\
  2 & 1
  \end{bmatrix}, \quad
  U = \begin{bmatrix}
  2 & 3 \\
  0 & 1
  \end{bmatrix}
  \]

  \textbf{Resolución del sistema:}

  Recordamos que \(Ax = b\) equivale a:
  \[
  LUx = b = \begin{bmatrix} 8 \\ 18 \end{bmatrix}
  \]
  Primero resolvemos \(Lz = b\):
  \[
  \begin{bmatrix}
  1 & 0 \\
  2 & 1
  \end{bmatrix}
  \begin{bmatrix}
  z_1 \\
  z_2
  \end{bmatrix}
  =
  \begin{bmatrix}
  8 \\
  18
  \end{bmatrix}
  \Rightarrow
  \begin{cases}
  z_1 = 8 \\
  2z_1 + z_2 = 18 \Rightarrow z_2 = 2
  \end{cases}
  \]
  Ahora resolvemos \(Ux = z\):
  \[
  \begin{bmatrix}
  2 & 3 \\
  0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2
  \end{bmatrix}
  =
  \begin{bmatrix}
  8 \\
  2
  \end{bmatrix}
  \Rightarrow
  \begin{cases}
  x_2 = 2 \\
  2x_1 + 3x_2 = 8 \Rightarrow x_1 = 1
  \end{cases}
  \]

  \textbf{Solución:} \(x_1 = 1\), \(x_2 = 2\)
\end{quote}

Para sistemas de mayor tamaño el procedimiento es el mismo. Se buscan las matrices \(L\) y \(U\) obteniendo la matriz \(LU\) y luego igualando este producto a \(A\). Una vez obtenidas, se resuelve el sistema.