\section{Espacios vectoriales}

\subsection{Cuerpo matemático}

Un cuerpo (o campo) \(K\) es, a grandes rasgos, un conjunto no vacío provisto de dos operaciones binarias, la suma \((+)\) y el producto \((\cdot)\), que satisfacen los siguientes requisitos:

\noindent \textbf{1. Estructura aditiva}
\begin{itemize}
  \item La suma es asociativa y conmutativa.
  \item Existe un elemento neutro aditivo, denotado \(0\), tal que \(a+0=0+a=a\) para todo \(a\in K\).
  \item Cada elemento \(a\in K\) tiene un inverso aditivo \(-a\) tal que \(a+(-a)=0\).
\end{itemize}

\noindent \textbf{2. Estructura multiplicativa}
\begin{itemize}
  \item El producto es asociativo y conmutativo.
  \item Existe un elemento neutro multiplicativo, denotado \(1\neq 0\), tal que \(a\cdot 1 = 1\cdot a = a\) para todo \(a\in K\).
  \item Cada elemento distinto de cero, \(a\neq 0\), tiene un inverso multiplicativo \(a^{-1}\) tal que \(a\cdot a^{-1}=1\).
\end{itemize}

\noindent \textbf{3. Distributividad}

El producto distribuye sobre la suma:
\[
  a\cdot(b+c) = a\cdot b + a\cdot c,\quad
  (a+b)\cdot c = a\cdot c + b\cdot c
  \quad\forall\,a,b,c\in K.
\]

En conjunto, estas propiedades garantizan que en \(K\) podemos manejar sumas, restas, productos y divisiones (salvo por cero) con la misma libertad aritmética que conocemos en \(\mathbb{R}\).

\ejemplo{ Cuerpos matemáticos habituales}
\begin{itemize}
  \item El cuerpo de los números reales, \(\mathbb{R}\).
  \item El cuerpo de los números complejos, \(\mathbb{C}\).
  \item El cuerpo de los números racionales, \(\mathbb{Q}\).
\end{itemize}

Cuando definimos un espacio vectorial, escogemos uno de esos cuerpos como conjunto de escalares; de este modo, todas las multiplicaciones por escalares vienen ``heredadas'' de la estructura de \(K\). Por ejemplo: En un espacio vectorial sobre \(\mathbb{R}\), cualquier \(k\in\mathbb{R}\) puede multiplicar a los vectores. En un espacio vectorial sobre \(\mathbb{C}\), permitimos escalares complejos, lo cual es fundamental, por ejemplo, en espacios de funciones de variable compleja.

Así, el cuerpo \(K\) proporciona el marco algebraico (con sus operaciones y axiomas) que garantiza que la multiplicación escalar en el espacio vectorial se comporte correctamente.

\subsection{Espacio vectorial}

Sea \textit{V} un conjunto no vacío, y \(\left(K, +, \cdot\right)\) un cuerpo (se trabajará con el cuerpo de los números reales y el cuerpo de los números complejos), se definen una operación binaria interna y una operación binaria externa en \textit{V} como sigue:
\begin{align*}
  +:& V \times V \rightarrow V \quad \text{tal que}~~ (u,v) \rightarrow u+v \\
  \cdot ~ :& K \times V \rightarrow V \quad \text{tal que}~~ (k,u) \rightarrow k\cdot u
\end{align*}

La notación:
\[
+\;:\;V\times V\longrightarrow V
\]
se interpreta de la manera siguiente:
\begin{itemize}
  \item \textbf{Dominio}: \(
  V \times V
  \) es el \textbf{producto cartesiano} de \(V\) consigo mismo. Un elemento de \(V \times V\) es un \textbf{par ordenado} donde \(u \in V\) y \(v \in V\). Decimos ``relación de \(V\) consigo mismo'' porque la suma toma dos vectores de \(V\).
  \item \textbf{Imagen o codominio}: \(V\) indica que el resultado de aplicar la operación ``\(+\)'' a cualquier par \((u,v)\) debe ser otro elemento de \(V\). Es la condición de \textit{clausura}: sumar dos vectores nos vuelve a dar un vector \hl{del mismo espacio}.
\end{itemize}

En términos coloquiales: ``\(+\) es una regla que, a cada par de vectores \((u,v)\), le asigna un único vector \(u+v\), y este resultado siempre pertenece a \(V\).''

De modo análogo, la multiplicación por escalar se escribe
\[
  \cdot\;:\;K\times V\;\longrightarrow\;V,
\]
donde:
\begin{itemize}
  \item El dominio \(K\times V\) agrupa un escalar \(k\in K\) y un vector \(u\in V\).
  \item El codominio \(V\) exige que el producto \(k\cdot u\) tenga como resultado siempre un vector en \(V\).
\end{itemize}
Así quedan definidas de manera precisa las dos operaciones básicas de cualquier espacio vectorial:
\begin{itemize}
  \item La suma toma dos vectores y los combina en uno solo.
  \item El producto escalar toma un escalar y un vector, y produce un vector.
\end{itemize}

Definir explícitamente estos mapeos te permite luego verificar los axiomas (asociatividad, conmutatividad, distributividad, existencia de neutros e inversos, etc.), pues todos ellos se expresan como igualdades entre resultados de estas dos funciones.

Se dice que \textit{V} con dichas operaciones es un \textit{K} espacio vectorial si se verifican los siguientes axiomas:
\begin{itemize}
  \item \textbf{A1} - Asociativa: Cualesquiera sean \(u,v\) y \(w\) de \(V\): \[
    (u+v)+w = u + (v+w)
  \]
  \item \textbf{A2} - Existencia de elemento neutro: Existe \(\vec{0}\) en \(V\) para todo \(u\) de \(V\):\[
    u+\vec{0}=\vec{0}+u = u
  \]
  \item \textbf{A3} - Existencia de elementos opuestos: Para todo \(u\) de \(V\) existe \(-u\) también en \(V\):\[
    u + (-u) = (-u) + u = \vec{0}
  \]
  \item \textbf{A4} - Conmutativa: Cualesquiera sean \(u\) y \(v\) de \(V\): \[
    u + v = v + u
  \]
  \item \textbf{A5} - Para todo \(k\) de \(K\), cualesquiera sean \(u\) y \(v\) de \(V\): \[
    k\cdot (u+v)=k\cdot u+k\cdot v
  \]
  \item \textbf{A6} - Cualesquiera sean \(k\) y \(k'\) de \(K\), para todo \(u\) de \(V\):\[
    (k + k')\cdot u = k \cdot u + k' \cdot u
  \]
  \item \textbf{A7} - Cualesquiera sean \(k\) y \(k'\) de \(K\), para todo \(u\) de \(V\):\[
    (k\cdot k')\cdot u = k \cdot (k' \cdot u)
  \]
  \item \textbf{A8} - Para todo \(u\) de \(V\): \[
    1 \cdot u = u \quad \text{(siendo 1 la unidad del cuerpo)}
  \]
\end{itemize}
A los elementos del espacio vectorial se los llama \hl{vectores}.

\begin{quote}
  \ejemplo{ Defina un espacio vectorial de los polinomios de grado menor o igual a dos}

  \textbf{Solución}: Para definir el espacio vectorial de polinomios de grado a lo sumo dos, procedemos del siguiente modo:

  Sea \(K\) el cuerpo de los números reales (o complejos) y consideremos el conjunto
  \[
  V = \bigl\{\,p(x) = a_0 + a_1x + a_2x^2 \;\big|\; a_0,a_1,a_2 \in K\bigr\}.
  \]
  En otras palabras, \(V\) es el conjunto de todas las funciones polinómicas cuyo grado es menor o igual a dos.

  A continuación, definimos en \(V\) las operaciones de suma y multiplicación por escalar exactamente como las de los polinomios:
  \begin{itemize}
    \item Para cualesquiera \(p(x),q(x)\in V\), la suma se define punto a punto:
    \[
      (p+q)(x) \;=\; p(x)+q(x) \;=\; \bigl(a_0+b_0\bigr) + \bigl(a_1+b_1\bigr)x + \bigl(a_2+b_2\bigr)x^2,
    \]
    donde \(p(x)=a_0+a_1x+a_2x^2\) y \(q(x)=b_0+b_1x+b_2x^2\).

  \item Para cualquier escalar \(k\in K\) y \(p(x)\in V\):
    \[
      (k\cdot p)(x) \;=\; k\,p(x) \;=\; (k\,a_0) + (k\,a_1)x + (k\,a_2)x^2.
    \]
  \end{itemize}

  Ahora bien, para ver que efectivamente \((V,+,\cdot)\) es un espacio vectorial sobre \(K\), basta observar que:
  \begin{enumerate}
    \item La suma de dos polinomios de grado \(\le2\) sigue siendo de grado \(\le2\) y lo mismo vale para la multiplicación por escalar.

    \item Asociatividad y conmutatividad de la suma, existencia del elemento neutro (el polinomio \(0(x)=0\)) y de inversos aditivos (para \(p(x)\), su inverso es \(-p(x)\)) se heredan directamente de las propiedades algebraicas de los coeficientes en \(K\).

    \item Distributividad de la multiplicación escalar respecto de la suma de polinomios y de escalares, compatibilidad de la multiplicación de escalares ( \((k\,k')p = k(k'p)\) ) y existencia del escalar unidad como operador neutro (\(1\cdot p = p\)) se verifican ecuación a ecuación, ya que se reducen a propiedades en el cuerpo \(K\).
  \end{enumerate}

  Por tanto, todos los axiomas A1 a A8 se cumplen en este caso concreto.
  \begin{tcolorbox}[myconclusion]
    Si no se está seguro de que todos los axiomas se cumplen (como en este caso), se puede demostrar cada uno de ellos para confirmarlo. Si los ocho axiomas se cumplen, entonces el conjunto es un espacio vectorial
  \end{tcolorbox}
\end{quote}

\begin{tcolorbox}[interesting_data, title=¿Puedo crear un conjunto cualquiera y verificar si es un espacio vectorial?]
  Exacto. El procedimiento general es precisamente ese: uno elige un conjunto \(V\) y un cuerpo de escalares \(K\) (por ejemplo \(\mathbb{R}\) o \(\mathbb{C}\)), define en \(V\) dos operaciones: la suma de dos elementos de \(V\) y la multiplicación de un escalar de \(K\) por un elemento de \(V\) y, si logra demostrar que estas operaciones satisfacen los ocho axiomas (A1-A8), entonces \((V,+,\cdot)\) es un espacio vectorial sobre \(K\). A partir de ese momento, cualquier objeto que pertenezca a \(V\) se denomina vector.

  No hay más requisitos ocultos: lo importante es que la suma y el producto por escalar estén bien definidos (es decir, que siempre produzcan un elemento de \(V\)) y que cumplan las propiedades de asociatividad, conmutatividad, existencia de neutros e inversos aditivos, distributividad y compatibilidad con la estructura del cuerpo.
\end{tcolorbox}

\paragraph{Consecuencias de la definición de espacio vectorial}

La definición de espacio vectorial implica que se cumplan las siguientes propiedades:
\begin{itemize}
  \item \textbf{P1}: \(0\cdot u = \vec{0} \qquad \forall u \in V\)
  \item \textbf{P2}: \(k \cdot \vec{0} = \vec{0} \qquad \forall k \in K\)
  \item \textbf{P3}: \((-1) \cdot u = -u \qquad \forall u \in V\)
\end{itemize}

\begin{tcolorbox}[mydanger]
  Cuidado: Es muy importante tener en cuenta que \[
  \text{Axiomas A1-A8} \quad \implies \quad \text{Propiedades P1-P3}
  \]
  Si solo se te dan P1, P2 y P3 sin garantizar que las operaciones están bien definidas, que la suma sea asociativa, que haya un neutro, o que se cumplan las reglas de distribución y compatibilidad, entonces no puedes concluir que estás en un espacio vectorial.
\end{tcolorbox}

\subsection{Conjunto de vectores: familia libre y familia ligada}

Un conjunto ordenado de vectores: \(F = \left\{u_1,u_2,\cdots,u_m\right\}\) de un espacio vectorial \(V(K)\) se denomina familia de vectores.

\subsubsection{Combinación lineal}

Una combinación lineal de los vectores de una familia con escalares \(a_1, a_2, \cdots, a_m\) es \textit{siempre} un vector del espacio vectorial \(V(K)\).
\[
  u = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m \qquad \forall u \in V
\]
Si el vector \(u\) de \(V\) es nulo, entonces la combinación lineal es:
\[
  \vec{0} = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m
\]
Esta combinación lineal es un sistema de ecuaciones lineales homogéneo, si al resolverlo resulta que los escalares son todos nulos \((a_1,a_2,\cdots,a_m = 0)\) entonces la familia de vectores \(F=\left\{u_1, u_2, \cdots, u_m \right\}\) recibe el nombre de \textbf{familia libre}.

En el caso contrario, si al menos uno de los escalares es no nulo, la familia \(F\) se denomina \textbf{ligada}.

Los vectores de una familia libre son linealmente independientes, esto significa que ninguno de ellos se puede expresar como combinación lineal de los demás de la familia. Los vectores de una familia ligada son linealmente dependientes.

\subsubsection{Propiedades de una familia libre}

Los vectores de una familia libre (linealmente independientes) presentan las siguientes propiedades:
\begin{enumerate}
  \item Si \(F\) es una familia libre, el vector nulo no pertenece a ella. \\ Demostración: Sea \(F=\left\{\vec{0},u_1, u_2, \cdots, u_m\right\}\) una familia de vectores \(V(K)\), en la combinación lineal: \[
    \vec{0} = a_0 \vec{0} + a_1 u_1 + a_2 u_2 + \cdots + a_m u_m
  \]
  Si suponemos que \(a_1, a_2, \cdots, a_m = 0\) resulta entonces \(\vec{0}=a_0 \vec{0}\). Pero por la propiedad \textbf{P2} estamos admitiendo que \(a_0\) no es necesariamente nulo, lo que implica que la familia no es libre.
  \item Si una familia de vectores de \(V\) consta de un solo vector no nulo, es una familia libre. \\ Demostración: Sea \(u_1 \neq \vec{0}\) y \(F=\left\{u_1\right\}\) una familia de vectores de \(V(K)\), la combinación lineal \(\vec{0} = a_1 u_1\) resulta que es verdadera si \(a_1 = 0\) o bien si \(u_1 = \vec{0}\). Sin embargo partimos de que \(u_1 \neq \vec{0}\), por lo que \(a_1 = 0\), lo que implica que \(F\) es una familia libre.
\end{enumerate}

\subsubsection{Propiedades de una familia ligada}

Los vectores de una familia ligada (linealmente dependientes) presentan las siguientes propiedades:
\begin{enumerate}
  \item Si el vector nulo pertenece a una familia \(F\) de vectores de \(V\), \(F\) es una familia ligada. (La demostración es análoga a la propiedad 1 del encabezado anterior).
  \item Si \(F\) es una familia ligada de vectores de \(V\), al menos uno de sus vectores se puede expresar como combinación lineal de los demás. \\ Demostración: Sea \(F=\left\{u_1, u_2, \cdots, u_m\right\}\) una familia ligada en \(V(K)\), y sea la combinación lineal del vector nulo:\[
    \vec{0} = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m
  \]
  Por ser \(F\) ligada, al menos uno de los escalares \(a_i \neq 0\). Supongamos que \(a_1 \neq 0\), entonces: \[
    \vec{0} = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m, ~~ a_1 \neq 0 \implies u_1 = -\frac{a_2}{a_1}u_2 - \cdots - \frac{a_m}{a_1}u_m
  \]
  Lo que nos está indicando que el vector \(u_1\) es combinación lineal de los demás vectores de la familia \(F\).
\end{enumerate}

\subsubsection{Familia generatriz}

Una familia de vectores de \(V(K): F=\left\{u_1,u_2,\cdots,u_m\right\}\) recibe el nombre de familia generatriz o generadora si \textit{todo} vector del espacio vectorial \(V\) se puede expresar como combinación lineal de los vectores de \(F\). Los vectores de \(F\) reciben el nombre de \textit{vectores generadores}.

Es decir, cualquiera sea \(u\) de \(V\), es posible expresarlo de la siguiente manera:\[
  u = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m
\]

\teorema{Dos vectores de un espacio vectorial \(V\) son linealmente dependientes (L.D.) si y sólo si uno de ellos es un múltiplo escalar del otro.}

\teorema{Sea \(K\) un cuerpo, y \(K^n\) el espacio vectorial de dimensión \(n\) sobre \(K\). Entonces, todo conjunto de \(n\) vectores linealmente independientes en \(K^n\) forma una base y, en consecuencia, genera todo \(K^n\).}
\label{teo:generador_li}

Por ejemplo, si \(K=\mathbb{R}\) (cuerpo de los números reales), el teorema se aplica como sigue: Todo conjunto de \(n\) vectores linealmente independientes (L.I.) en \(\mathbb{R}^n\) genera \(\mathbb{R}^n\)

\subsection{Base y dimensión}

Una familia de vectores \(F = \left\{u_1, u_2, \cdots, u_m\right\}\) de \(V(K)\) se denomina \hl{base} de \(V\) si es a la vez familia \textbf{libre y generatriz}.

Esto equivale a decir que todo \(u \in V\) se puede expresar como combinación lineal de los vectores de \(F\), o bien:
\[
  u = a_1 u_1 + a_2 u _2 + \cdots + a_m u_m
\] 
y además si \(u\) es el vector nulo entonces:
\[
  \vec{0} = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m \qquad a_i = 0; ~~ i=1,2,\cdots,m
\]
es decir, los vectores de \(F\) son L.I.

Hemos visto en el teorema \ref{teo:generador_li} que todo conjunto de \(n\) vectores linealmente independientes (L.I.) en \(\mathbb{R}^n\) genera a \(\mathbb{R}^n\). Entonces, de acuerdo a la definición de base, se tiene que:
\begin{tcolorbox}
  \centering
  Todo conjunto de \(n\) vectores L.I. en \(K^n\), constituye una base de \(K^n\)
\end{tcolorbox}

Si el espacio vectorial \(V\) tiene una base finita, es decir, con finitos elementos, entonces la \textbf{dimensión} de \(V\) que se denota \(\text{dim}V\), es el número de vectores que tiene una base cualquiera de \(V\). Este último recibe el nombre de \textit{espacio vectorial de dimensión finita}. En cualquier otro caso, se dice que \(V\) es un espacio vectorial de dimensión infinita.

\begin{itemize}
  \item Todo vector de \(V\) se expresa de manera única en cada base.
  \item El número de vectores de una base de \(V\) se denomina \hl{dimensión} del espacio vectorial.
  \item Todas las bases de un espacio vectorial de dimensión finita \textit{n} tienen exactamente \textit{n} vectores.
  \item Existen espacios vectoriales de dimensión infinita.
  \item El espacio vectorial \(\left\{\vec{0}\right\}\) tiene dimensión cero por definición.
  \item Si \textit{V} es un espacio vectorial de dimensión finita \textit{n}:
  \begin{enumerate}
    \item \(n+1\) vectores de \(V\) son linealmente dependientes, lo que equivale a decir que una familia libre tiene a lo más \(n\) elementos (ver teorema \ref{teo:base_ld}).
    \item una familia generatriz tiene como mínimo \textit{n} elementos (ver teorema \ref{teo:elementos_de_una_base}).
    \item toda familia libre de \(V\) con \(n\) elementos es una base de \(V\).
    \item toda familia generatriz de \(V\) con \(n\) elementos es una base de \(V\).
  \end{enumerate}
\end{itemize}

\teorema{Si \(\left\{v_1, v_2, \cdots, v_n\right\}\) es una base de un espacio vectorial \(V\), entonces todo conjunto con más de \(n\) vectores es linealmente dependiente (L.D.).}
\label{teo:base_ld}

\teorema{Si \(\left\{u_1, u_2, \cdots, u_m\right\}\) y \(\left\{v_1, v_2, \cdots, v_n\right\}\) son bases del espacio vectorial \(V\), entonces \(m=n\)}
\label{teo:elementos_de_una_base}

\subsubsection{Componentes de un vector relativas a una base}

Sea \(V(K)\) un espacio vectorial de dimensión \(m\) y \(F = \left\{u_1,u_2,\cdots,u_m\right\}\) una base de \(V\), un vector \(u\) cualquiera de \(V\) se expresa de manera única:
\[
  u = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m
\]
Los escalares que permiten esta combinación lineal reciben el nombre de componentes del vector \(u\) y se puede anotar asi:
\[
  u = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m = \begin{pmatrix}
    a_1 \\ a_2 \\ \vdots \\ a_m
  \end{pmatrix}
\]
El vector de coordenadas de \(u\) relativo a la base \(B\) se denota \((u)_B\):
\[
  (u)_B = (a_1, a_2, \cdots, a_m)
\]
Si \(v\) es otro vector de \(V\) se expresa en \(B\) de la siguiente manera:
\[
  v = b_1 u_1 + b_2 u_2 + \cdots + b_m u_m = \begin{pmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_m
  \end{pmatrix}
\]
Las componentes del vector \(u+v\) son las suma de las componentes de \(u\) más las componentes de \(v\), y las componentes del vector \(k\cdot u\) son las que se obtienen de multiplicar \(k\) por cada una de las componentes de \(u\). De forma explícita:
\begin{align*}
  u + v &= (a_1 u_1 + a_2 u_2 + \cdots + a_m u_m) + (b_1 u_1 + b_2 u_2 + \cdots + b_m u_m) \\[3pt]
  u + v &= \begin{pmatrix}
    a_1 \\ a_2 \\ \vdots \\ a_m
  \end{pmatrix} + \begin{pmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_m
  \end{pmatrix} = \begin{pmatrix}
    a_1 + b_1 \\ a_2 + b_2 \\ \vdots \\ a_m + b_m
  \end{pmatrix} \\[10pt]
  k\cdot u &= k \cdot \begin{pmatrix}
    a_1 \\ a_2 \\ \vdots \\ a_m
  \end{pmatrix} = \begin{pmatrix}
    k \cdot a_1 \\ k \cdot a_2 \\ \vdots \\ k \cdot a_m
  \end{pmatrix}
\end{align*}
Es decir, si tienes dos vectores expresados en coordenadas relativas a una base:
\[
u = a_1 u_1 + \dots + a_m u_m,\quad
v = b_1 u_1 + \dots + b_m u_m,
\]
Entonces las coordenadas relativas del vector \(u + v\) son:
\[
(u+v)_B = (a_1 + b_1,\; a_2 + b_2,\; \dots,\; a_m + b_m)
\]
Y si multiplicas \(u\) por un escalar \(k\), las coordenadas se escalan:
\[
(k\cdot u)_B = (k a_1,\; k a_2,\; \dots,\; k a_m)
\]
Esto es una consecuencia directa de la linealidad del espacio vectorial: suma y producto por escalar se reflejan componente a componente.

\begin{quote}
  \ejemplo{ Supongamos que tienes un espacio vectorial \(V(K)\), por ejemplo \(V = \mathbb{R}^2\), y eliges una base \(B = \{u_1, u_2\}\), donde los vectores \(u_1\) y \(u_2\) son linealmente independientes.}

  Entonces, cualquier vector \(v \in \mathbb{R}^2\) puede escribirse de forma única como combinación lineal:
  \[
  v = a_1 u_1 + a_2 u_2
  \]
  Los escalares \(a_1\) y \(a_2\) se llaman coordenadas del vector \(v\) relativas a la base \(B\). Veamos un ejemplo concreto.

  Tomemos una base no canónica de \(\mathbb{R}^2\):
  \[
  B = \left\{ u_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \quad
  u_2 = \begin{pmatrix} -1 \\ 2 \end{pmatrix} \right\}
  \]
  Y el vector:
  \[
  v = \begin{pmatrix} 2 \\ 5 \end{pmatrix}
  \]
  Queremos expresar \(v\) como combinación lineal de \(u_1\) y \(u_2\):
  \[
  v = a_1 u_1 + a_2 u_2
  \]
  Sustituyendo:
  \[
  \begin{pmatrix} 2 \\ 5 \end{pmatrix}
  =
  a_1 \begin{pmatrix} 1 \\ 1 \end{pmatrix}
  + a_2 \begin{pmatrix} -1 \\ 2 \end{pmatrix}
  =
  \begin{pmatrix} a_1 - a_2 \\ a_1 + 2a_2 \end{pmatrix}
  \]
  Entonces, debemos resolver el sistema:
  \[
  \begin{cases}
  a_1 - a_2 = 2 \\
  a_1 + 2a_2 = 5
  \end{cases}
  \]
  Resolviendo:
  \begin{itemize}
    \item De la primera: \(a_1 = 2 + a_2\)
    \item Sustituyendo en la segunda:
    \[
    (2 + a_2) + 2a_2 = 5 \quad\Rightarrow\quad 2 + 3a_2 = 5 \quad\Rightarrow\quad a_2 = 1,\; a_1 = 3
    \]
  \end{itemize}
  Entonces,
  \[
  v = 3u_1 + 1u_2
  \quad\Longrightarrow\quad
  (v)_B = \begin{pmatrix} 3 \\ 1 \end{pmatrix}
  \]
  Esto significa que respecto de la base \(B\), el vector \(v\) se representa mediante las coordenadas \((3, 1)\), aunque geométricamente sigue siendo el mismo vector \(\begin{pmatrix} 2 \\ 5 \end{pmatrix}\) en el plano.
\end{quote}

\begin{tcolorbox}
  Cuando dices que un vector tiene coordenadas relativas a una base \(B\), estás indicando cómo se escribe ese vector como combinación lineal de los vectores de la base \(B\). Esta representación depende totalmente de la base elegida, y por eso el mismo vector puede tener coordenadas distintas en diferentes bases.
\end{tcolorbox}

\subsubsection{Cambio de base}

\paragraph{¿Qué significa cambiar de base?}

En un espacio vectorial como \(\mathbb{R}^2\), cada vector \(w\) puede escribirse de muchas maneras geométricas, pero algebraicamente su representación depende de la base que uses.

Lo desarrollaremos para un espacio vectorial de dimensión 2 y luego lo generalizaremos a un espacio de dimensión finita \textit{n}.

\paragraph{Cambio de base en \(\mathbb{R}^2\)}

Sea \(V(K)\) un espacio vectorial y \(F_1 = \left\{u_1, u_2\right\}\) y \(F_2 = \left\{v_1, v_2\right\}\) dos bases de \(V\), un vector \(w\) de \(V\) se expresa de manera única en \(F_1\) y de manera única en \(F_2\).

Entonces, las dos bases de \(\mathbb{R}^2\) son:
\begin{itemize}
  \item \(F_1 = \{u_1, u_2\}\)
  \item \(F_2 = \{v_1, v_2\}\)
\end{itemize}
Cada vector \(w\in \mathbb{R}^2\) tiene una única representación lineal respecto de cada base:
\[
w = \alpha_1 u_1 + \alpha_2 u_2 = \beta_1 v_1 + \beta_2 v_2
\]
Entonces:
\begin{itemize}
  \item El vector columna \(\begin{pmatrix} \alpha_1 \\ \alpha_2 \end{pmatrix}\) representa a \(w\) en la base \(F_1\).
  \item El vector columna \(\begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}\) representa a \(w\) en la base \(F_2\).
\end{itemize}
El objetivo es pasar de una representación a otra, es decir, dado el vector \(w\) expresado en la base \(F_2\), ¿cómo se obtiene su representación en la base \(F_1\)? Y viceversa.

Para esto, se usa la relación entre las bases. Si conoces cómo se expresan los vectores de \(F_2\) (es decir, \(v_1\) y \(v_2\)) en la base \(F_1\), puedes construir la llamada matriz de cambio de base o matriz de pasaje.

Como sabemos cómo expresar un vector \(w\) en \(F_1\) y en \(F_2\), vemos que
\begin{gather*}
  (w)_{F1} = \alpha_1 u_1 + \alpha_2 u_2 = \begin{pmatrix}
    \alpha_1 \\ \alpha_2
  \end{pmatrix} \qquad (w)_{F2} = \beta_1 v_1 + \beta_2 v_2 = \begin{pmatrix}
    \beta_1 \\ \beta_2
  \end{pmatrix} \\[5pt]
  w = \alpha_1 u_1 + \alpha_2 u_2 = \beta_1 v_1 + \beta_2 v_2
\end{gather*}
Si los vectores \(v_1\) y \(v_2\) de la base \(F_2\) se escriben como combinaciones lineales de la base \(F_1 = \{u_1, u_2\}\):
\[
  v_1 = \begin{pmatrix}
    a_{11} \\ a_{21}
  \end{pmatrix} = a_{11} u_1 + a_{21} u_2 \qquad \text{y} \qquad v_2 = \begin{pmatrix}
    a_{12} \\ a_{22}
  \end{pmatrix} = a_{12} u_1 + a_{22} u_2
\]
entonces podemos escribir el vector \(w\) como:
\[
w = \beta_1 v_1 + \beta_2 v_2 = \beta_1(a_{11}u_1 + a_{21}u_2) + \beta_2(a_{12}u_1 + a_{22}u_2),
\]
agrupando términos:
\[
w = (\beta_1 a_{11} + \beta_2 a_{12}) u_1 + (\beta_1 a_{21} + \beta_2 a_{22}) u_2,
\]
y comparando con la expresión \(w = \alpha_1 u_1 + \alpha_2 u_2\), se deduce que:
\[
\begin{cases}
\alpha_1 = \beta_1 a_{11} + \beta_2 a_{12} \\
\alpha_2 = \beta_1 a_{21} + \beta_2 a_{22}
\end{cases}
\quad\Longrightarrow\quad
\begin{pmatrix}
\alpha_1 \\
\alpha_2
\end{pmatrix}
=
\begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix}
\begin{pmatrix}
\beta_1 \\
\beta_2
\end{pmatrix}
\]
Que es un sistema de dos ecuaciones con dos incógnitas, con solución única, ya que dijimos que cada vector se expresa de manera única en cada base. Resolver el sistema equivale a hallar las componentes de \(w\) en \(F_2\) a partir de \(F_1\) o viceversa.

En forma matricial lo podemos expresar así:
\begin{align*}
  X = P \cdot X' ~ : ~ \begin{pmatrix}
    \alpha_1 \\ \alpha_2
  \end{pmatrix} = \begin{pmatrix}
    a_{11} & a_{12} \\ 
    a_{21} & a_{22}
  \end{pmatrix} \cdot \begin{pmatrix}
    \beta_1 \\ \beta_2
  \end{pmatrix}
\end{align*}
Entonces se define la \hl{matriz de pasaje} de \(F_2\) a \(F_1\) como:
\[
P = 
\begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix}
\]
donde cada columna de \(P\) contiene las coordenadas de un vector de \(F_2\) expresado en la base \(F_1\), en este caso \((v_1)_{F1} = (a_{11}, a_{21})\) y \((v_2)_{F1} = (a_{12}, a_{22})\).

\newpage

\paragraph{¿Y para ir de \(F_1\) a \(F_2\)?}

Si quisieras obtener las coordenadas de \(w\) en \(F_2\) a partir de las coordenadas en \(F_1\), simplemente debes invertir la matriz:
\[
X' = P^{-1} \cdot X
\]

\paragraph{Generalización}

Todo esto se extiende sin dificultad a espacios \(K^n\) de dimensión mayor a 2: las matrices de pasaje serán de tamaño \(n\times n\), y cumplirán la misma lógica: cada columna de la matriz contiene un vector de una base expresado en la otra base.

\subsection{Subespacio vectorial}

Sea \(V(K)\) un espacio vectorial y \(S\) un subconjunto no vacío de \(V\), si \(S\) es espacio vectorial sobre \(K\) respecto a las mismas operaciones definidas en \(V\), es un subespacio de \(V\). En otras palabras: \(S(K)\) subespacio de \(V(K)\)

\paragraph{Parte estable (propiedad)}

Un subconjunto \(S\) no vacío de \(V(K)\) es estable para las combinaciones lineales si se verifica que para cualesquier par de elementos \(u,v\) de \(S\) y para cualquier escalar \(k\) de \(K\), los vectores \(u+v\) y \(k\cdot u\) pertenecen también a \(S\). \(S\) recibe el nombre de parte estable.
\[
  S ~ \text{es parte estable} ~ \Longleftrightarrow ~ S \subset V \land S \neq \emptyset \land \begin{cases}
    u + v \in S \quad \forall u, v \in S\\
    k \cdot u \in S \quad \forall k \in K \land \forall u \in S
  \end{cases}
\]
Se puede demostrar que si ``\(S\) es parte estable de \(V(K)\) es un subespacio vectorial de \(V\)''.

Por tanto, para comprobar que \(S\) es parte estable basta con verificar:
\begin{enumerate}
  \item \(S\neq\emptyset\) (o, mejor, que \(\mathbf{0}\in S\)).
  \item Si \(u,v\in S\) entonces \(u+v\in S\).
  \item Si \(u\in S\) y \(k\in K\) entonces \(k\cdot u\in S\).
\end{enumerate}

Con eso ya queda demostrado que \(S\) hereda todos los axiomas de espacio vectorial y es, en consecuencia, un subespacio de \(V\).


\subsubsection{Subespacios triviales}

Sea \(V(K)\) un espacio vectorial, los siguientes reciben el nombre de \textit{subespacios triviales} o impropios: \(\left\{\vec{0}\right\} (K) ~~ \text{y} ~~ V(K)\), es decir el conjunto formado exclusivamente por el vector nulo y el mismo \(V\), cualquier otro subespacio de \(V\) recibe el nombre de subespacio propio.

Todo espacio vectorial admite al menos los subespacios triviales.

\subsubsection{Dimensión de los subespacios}

Sea el espacio vectorial \(V(K)\), de dimensión finita \(n\), todo subespacio de \(V\) tiene dimensión finita \(m\) tal que \(m \leq n\).

\begin{itemize}
  \item Por convención el subespacio trivial \(U=\left\{\vec{0}\right\}\) como carece de base, entonces se dice que su dimensión es cero, \(\text{dim}\left\{\vec{0}\right\}=0\), en este caso el único subespacio es él mismo, por lo tanto mantiene la dimensión nula.
  \item Si \(\text{dim }V=1\), admite subespacios de dimensión 0 y dimensión 1. Esto significa que sólo admite a los subespacios triviales, ya que no existe otro subespacio de la misma dimensión incluido él.
  \item Si \(\text{dim }V = 2\), admite los subespacios triviales, de dimensión 0 y de dimensión 2, pero también admite subespacios de dimensión 1.
  \item En general si \(\text{dim }V=n\) admite a los subespacios triviales de dimensión 0 y \textit{n} y además todos los de dimensión \(m\) tal que \(0 \leq m \leq n\)
\end{itemize}